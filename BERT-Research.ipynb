{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a3cf7c",
   "metadata": {},
   "source": [
    "\n",
    "# Hablemos de BERT Red Neuronal\n",
    "\n",
    "<img src=\"figures/bert_logo.jpeg\" alt=\"bert-logo\" style=\"float:center;width:300;height:200;\">\n",
    "\n",
    "**Autores**\n",
    "- Erika Ortiz  \n",
    "- Romina Soledad Iglesias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5427963c",
   "metadata": {},
   "source": [
    "## Palabras clave\n",
    "\n",
    "- **BERT**: Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "- **PLN:** Procesamiento de Lenguaje Natural  (NLP en Inglés): Es un campo de las ciencias de la computación, de la inteligencia artificial y de la lingüística que estudia las interacciones entre las computadoras y el lenguaje humano. Se ocupa de la formulación e investigación de mecanismos eficaces computacionalmente para la comunicación entre personas y máquinas por medio del lenguaje natural, es decir, de las lenguas del mundo.\n",
    "\n",
    "- **Deep Learning:** Aprendizaje profundo (en inglés, deep learning) es un conjunto de algoritmos de aprendizaje automático (en inglés, machine learning) que intenta modelar abstracciones de alto nivel en datos usando arquitecturas computacionales que admiten transformaciones no lineales múltiples e iterativas de datos expresados en forma matricial o tensorial. \n",
    "\n",
    "* **Machine learning:** es una disciplina, dentro de la Inteligencia Artificial, donde se estudian métodos para hacer predicciones, que podamos programar y automatizar. Machine Learning dota a las máquinas de la capacidad de aprender.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4acfcbc",
   "metadata": {},
   "source": [
    "## Introducción\t\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) es un modelo de machine learning de código abierto desarrollado por investigadores de Google en 2018. Se creó con el objetivo de mejorar el procesamiento de lenguaje natural y, de esta manera, lograr que el motor de búsqueda de Google interprete las consultas de una manera más precisa.\n",
    "\n",
    "Su funcionamiento está basado en las redes neuronales Transformers, desarrolladas por Google en 2017, las cuales son una mejora de los modelos de lenguaje utilizados hasta entonces.\n",
    "\n",
    "El modelo BERT puede ser utilizado en diferentes aplicaciones como traducción de textos, responder preguntas, generación de documentos, desambiguación del sentido de las palabras, entre otras. y, a partir de él, se están generando un gran número de nuevas aplicaciones.\n",
    "\n",
    "Este trabajo busca explicar el concepto de BERT, en que se basa su funcionamiento y como funciona. Se detallarán algunos de los desarrollos que se realizaron a partir de BERT y, por último, se hará una presentación de una de las aplicaciones de BERT. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f3ec91",
   "metadata": {},
   "source": [
    "## Objetivos \n",
    "\n",
    "1. Explicar el concepto de BERT. \n",
    "2. Identificar las aplicaciones del modelo BERT. \n",
    "3. Presentar un demo de algunas de las aplicaciones de BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946ccb95",
   "metadata": {},
   "source": [
    "## Resultados de la Investigacion\n",
    "\n",
    "### ¿Qué es BERT?\n",
    "\n",
    "BERT es un modelo de machine learning de código abierto creado y publicado en 2018 por Jacob Devlin y sus compañeros en Google con el objetivo de mejorar el procesamiento de lenguage natural y ser utilizado para optimizar los resultados del motor de búsqueda de Google, BERT esta basado en el uso  de redes neuronales Transformers. El modelo BERT original se creó usando dos corpus de lengua inglesa: BookCorpus y Wikipedia en inglés.\n",
    "\n",
    "BERT por sus siglas en Inglés ***Bidirectional Encoder Representations from Transformers*** funciona de manera bidireccional, es decir, analiza los textos ya sea de derecha a izquierda como de izquierda a derecha, esto permite que no solo reconozca los términos claves de una búsqueda, sino que entienda el contexto de las palabras en la frase u oración, interpretando las búsquedas de manera más precisa. \n",
    "\n",
    "Por ejemplo: En la figura 1, se puede observar un ejemplo de cómo el algoritmo aplica la bidireccionalidad. En este caso la palabra clave **cura** presenta dos significados diferentes que dependen del contexto que la acompañe. En la primera oración lo que determina su significado se encuentra en la parte anterior, mientras que en la segunda oración el significado queda determinado por la palabra que la continua.\n",
    "\n",
    "<div>\n",
    "<img src=\"figures/ejemplo_bidireccionalidad.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Esta nueva capacidad de BERT de bidireccionalidad permite entender mejor el lenguage natural y mejorar el resultado de búsquedas basandose no solo en Keyworkds sino utilizando todo el contexto de la frase, por ejemplo en búsquedas antes de BERT en el año 2019 un resultado para la frase: **\"2019 turista brasileño a EE.UU. necesita visa\"** arrojaba diferentes resultados antes y despues de integrar BERT en el algoritmo de búsqueda de Google como se observa en la tabla:\n",
    "\n",
    "<div class=\"container\">\n",
    "  <table class=\"table\", style=\"width:80%\" >\n",
    "    <thead>\n",
    "      <tr>\n",
    "        <th style=\"width:30%\">Query</th>\n",
    "        <th style=\"text-align: center\">Search Engine Results Before and After using BERT</th>\n",
    "      </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "      <tr>\n",
    "        <td style=\"text-align: left\">\"2019 brazil traveler to usa need a visa.”*1</td>\n",
    "        <td><img src=\"figures/search_result_ex1.png\" /></td>\n",
    "      </tr>  \n",
    "      <tr>\n",
    "        <td style=\"text-align: left\">“do estheticians stand a lot at work.”*2</td>\n",
    "        <td><img src=\"figures/search_result_ex2.png\"/></td>\n",
    "      </tr>  \n",
    "    </tbody>\n",
    "    <tfoot>\n",
    "      <tr>\n",
    "        <td style=\"text-align: left\" colspan=\"2\"> *1 La palabra \"to\" era ignorado por el motor de búsqueda cambiando el contexto de la frase por lo tanto sin esa preposición el algoritmo arrojaba resultados de ciudadanos estadounidenses viajando a Brazil y no lo contrario que era un ciudadano brasilero viajando a los Estados Unidos. \n",
    "        </td>\n",
    "      </tr>\n",
    "      <tr>  \n",
    "          <td style=\"text-align: left\" colspan=\"2\"> *2 El algoritmo de Google antes de BERT tenia un enfoque basado en keywords, en este caso el texto \"stand-alone\" estaba enlazado con la palabra \"stand\" pero no lo relacionaba con el esfuerzo fisico del trabajo de una esteticista,  con el modelo BERT gracias a la bidireccioonalidad y entender el contexto esta relacion es mas clara por lo tanto el resultado es mas ajustado a lo que el usuario esta buscando usando el lenguaje natural. \n",
    "        </td>\n",
    "      </tr>\n",
    "    </tfoot>\n",
    "  </table>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "Es importante destacar que BERT no sustituye por completo al anterior algoritmo, ***RankBrain*** (el primero en el que Google introdujo la inteligencia artificial), más bien lo complementa. En este sentido, BERT afecta a aquellas búsquedas más complejas, las que pueden depender del contexto o el tono.  Con Google BERT, el algoritmo podrá identificar correctamente el sentido de la búsqueda, mostrando resultados más acordes a la misma.\n",
    "\n",
    "El 25 de octubre de 2019, Google Search anunció que habían comenzado a aplicar modelos BERT para consultas de búsqueda en inglés dentro de Estados Unidos. El 9 de diciembre de 2019, se informó que BERT había sido integrado a Google Search para más de 70 idiomas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d9932a",
   "metadata": {},
   "source": [
    "### Redes Neuronales Transformers\n",
    "\n",
    "BERT basa su funcionamiento en las redes neuronales Transformers. Dichas redes son modelos que tienen diversas aplicaciones como traducir textos, escribir poemas, sintetizar y/o generar texto, generar código fuente entre otras aplicaciones. \n",
    " \n",
    "Las redes transformers fueron creados por Google en  2017 y por la Universidad de Toronto. En el momento de su introducción, los modelos de lenguaje usaban principalmente redes neuronales recurrentes (RNN) y redes neuronales convolucionales (CNN) para manejar tareas de Procesamiento de Lenguaje Natural (PNL).\n",
    "Estos modelos aún son competentes, sin embargo, el Transformer se considera una mejora significativa ya que no requiere que las secuencias de datos se procesen en un orden fijo, mientras que las RNN y las CNN sí lo hacen. \n",
    "\n",
    "Su enfoque principal de los Transformers era realizar traducciones, pero puede ser entrenado para diferentes usos ya que poseen una alta capacidad de procesamiento en paralelo lo que los hace más eficientes reduciendo el tiempo y facilidad de entrenamiento y aumentando el tamaño de los set de datos que pueden procesar. \n",
    " \n",
    "Los Transformers están basados en tres principales innovaciones: \n",
    "- Positioning encoding\n",
    "- Attention\n",
    "- Self attention. \n",
    "\n",
    "A continuación explicamos en qué consiste cada una de las tres principales innovaciones:\n",
    " \n",
    "- **Positioning Encoding**\n",
    "\n",
    "Los Transformers utilizan codificadores posicionales para etiquetar elementos de datos que entran y salen de la red. \n",
    "\n",
    "El Transformer recibe una oración de entrada y la convierte en dos secuencias: una secuencia de vectores de palabras y una secuencia de codificaciones posicionales. Ambos vectores son escritos usando representaciones numéricas del texto para que la red neuronal pueda procesarlas. Cada palabra del diccionario se representa como un vector. Las codificaciones posicionales son una representación vectorial de la posición de la palabra en la oración original.\n",
    "\n",
    "El Transformer junta ambas secuencias y pasa el resultado a través de una serie de codificadores, seguidos de una serie de decodificadores. Esto es necesario debido a que el input no es alimentado en la red de forma secuencial sino que se pasa todo de una vez.\n",
    "\n",
    "Referencia: https://www.linkedin.com/pulse/transformers-redes-neuronales-cristian-santander/?originalSubdomain=es\n",
    "\n",
    " \n",
    "- **Attention**\n",
    "\n",
    "Attention es un mecanismo que permite a un modelo de texto «mirar» cada una de las palabras de la frase original al tomar una decisión sobre cómo traducir las palabras de la frase de salida.\n",
    " \n",
    "Este modelo aprende de los datos de entrenamiento. Por ejemplo, en el caso de los traductores, al ver miles de ejemplos de frases en diferentes idiomas, el modelo aprende qué tipos de palabras son interdependientes. De esta manera aprende a respetar el género, la pluralidad y otras reglas gramaticales.\n",
    "\n",
    "\n",
    "Referencia: https://www.ibidemgroup.com/edu/traduccion-automatica-gpt3-bert-t5/\n",
    " \n",
    "- **Self attention**\n",
    " \n",
    "El mecanismo Self attention es lo que permite al modelo saber con qué otra palabra de la oración está relacionada la palabra que se procesa en ese instante de tiempo. \n",
    "\n",
    "Es una capa que ayuda al codificador a ver otras palabras en la oración de entrada mientras codifica una palabra específica.\n",
    "\n",
    "Referencia: https://e-archivo.uc3m.es/bitstream/handle/10016/32792/TFG_Iago_Collarte_Gonzalez.pdf?sequence=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95571e6d",
   "metadata": {},
   "source": [
    "### Arquitectura del modelo de transformadores \n",
    "\n",
    "<div>\n",
    "<img src=\"figures/transformers-architecture.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "<cite data-footcite=\"Figure1\">Source: Figure 1: The Transformer - model architecture. Wasani et al.., 2017 [Paper attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)</cite>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed342fa6",
   "metadata": {},
   "source": [
    "### Cómo funciona BERT?\n",
    "\n",
    "Como ya mencionamos anteriormente, BERT esta basado en **Transformers**, el mecanismo de atención que permite que se comprenda el contexto y se entienda mejor las relaciones entre palabras dentro de una frase u oración. \n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"figures/bert_working.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "<cite data-footcite=\"Figure1\">Figura: BERT Explained Wasani et al.., 2017 [BERT Explained a complete guide](https://medium.com/@samia.khalid/bert-explained-a-complete-guide-with-theory-and-tutorial-3ac9ebc8fa7c)</cite>.\n",
    "\n",
    "En la imagen observamos dos frases que van a a ser utilizadas como entrada en BERT, por lo tanto podemos observar que una secuencia de tokens es convertida en vectores para posteriormente ser procesada en la red neuronal. A continuación explicamos que significa cada etapa. \n",
    "\n",
    "1. **Token embeddings:** El token [CLS] se agrega al inicio de la primera frase y el token [SEP] se agrega al final de cada frase u oración. (ver figura adjunta) \n",
    "2. **Segment embeddings:** Se agrega un indicador que identifique la frase A y la frase B, Esta identificación permite que el codificador distinga las dos frases A y B. \n",
    "3. **Positional embeddings:** Un indicador de posición es agregado a cada token para indicar su posición en la frase. \n",
    "\n",
    "\n",
    "#### Fases de preentrenamiento de BERT**\n",
    "\n",
    "A continuación explicamos las fases de pre-entrenamiento del modelo BERT: \n",
    "\n",
    "- **Dataset de entrenamiento**\n",
    "\n",
    "BERT fue entrenado con un dataset de mas de 3.3 billones de palabras tomados de Wikipedia (~2.5B words) y Google BooksCorpus (~800M words), esto ha contribuido a su inmenso éxito para entender el lenguage natural. \n",
    "\n",
    "Procesar esa cantidad de datos requiere tener un gran poder computacional, por lo tanto BERT tomo ventaja de la arquitectura de tansformers y  la velocidad de procesamiento de las TPUs (Tensor Processing Units, Google construyó este circuito exclusivamente para procesamiento de modelos de ML). \n",
    "\n",
    "Para entrenar BERT se utilizaroon 64 TPUs durante 4 dias. \n",
    "\n",
    "- **Mask Language Model (MLM)**\n",
    "Consiste en enmascar algunas de las palabras de la entrada y luego se condiciona cada palabra bidireccionalmente para predecir las palabras enmascaradas. Para ello, al tomar una oración, el modelo enmascara al azar el 15% de las palabras en la entrada. Luego, el modelo intenta predecir el valor original de las palabras enmascaradas, basándose en el contexto proporcionado por las otras palabras no enmascaradas de la secuencia.\n",
    "\n",
    "Por ejemplo:  “Dang! I’m out fishing and a huge trout just [MASK] my line!\", el modelo predice la palabra que iria en el MASK dependiendo del contexto obtenido de la frase en este caso podria decir algo como **Broke**.  Los humanos por naturaleza utilizamos el masking es parte natural del uso del lenguage, para nosotros como humanos esta tarea es trivial pero lograr que una máquina logre o tenga la capacidad de realizar esta tarea es parte de lo que el modelo BERT está logrando.  \n",
    "\n",
    "- **Predicción de la siguiente oración (Next Sentence Prediction, NSP)**\n",
    "En esta etapa, BERT aprende a modelar las relaciones entre oraciones. Para ello, en el proceso de entrenamiento, el modelo recibe pares de oraciones como entrada y aprende a predecir si la segunda oración del par es la oración posterior del documento original. Es decir, si se tienen dos oraciones A y B, BERT debe descifrar si B es la siguiente oración real que viene después de A en el corpus, o es solo una oración aleatoria. Por ejemplo: \n",
    "            Erika fue al supermercado. Ella compró un delicioso postre. (Correct NSP)\n",
    "            Romina preparó café. El helado de vainilla es el mejor. (Incorrect NSP)\n",
    "\n",
    "Al entrenar el modelo BERT, ambas técnicas se entrenan juntas, de esta forma, se minimiza la función de pérdida combinada de las dos estrategias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc74fd5a",
   "metadata": {},
   "source": [
    "### Aplicaciones de BERT\n",
    "\n",
    "El modelo BERT puede ayudar a resolver problemas de procesamiento de lenguage natural como: \n",
    "\n",
    "- ***Análisis de Sentimientos:*** Puede analizar y determinar si una critica es positiva o negativa, clasificar el email entre spam y no spam.\n",
    "\n",
    "- ***Responder Preguntas:*** Por ejemplo chatbox que responden preguntas. \n",
    "\n",
    "- ***Traducción de textos:*** Por ejemplo google translate utiliza BERT para realizar la traducción de textos. \n",
    "\n",
    "- ***Predicción de texto:*** Por ejemplo Gmail puede predecir el siguiente texto cuando estas redactando un email. \n",
    "\n",
    "- ***Resumen de textos:*** Google search engine utiliza BERT y en los resultados de búsquda cuando te aparece un resumen de un resultado especifico es también parte de lo que BERT puede hacer. \n",
    "\n",
    "- ***Generación de textos:*** Puede escribir un articulo acerca de cualquier tema con solo unas pocas frases como entrada. \n",
    "\n",
    "- ***Resolución de polisemia y correferencia:*** Diferenciar entre palabras que suenan o se ven iguales pero tienen diferentes significados, por ejemplo la palabra **Banco** que dependediendo del contexto de la oración puede tomar diferentes significados, aqui algunos ejemplos: \n",
    "                    \n",
    "            ¡No te sientes en ese **banco** que está roto! \n",
    "            \"El **banco** central de mi país subió las tasas de intereses de todas sus operaciones\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bae6cb",
   "metadata": {},
   "source": [
    "#### BERT vs Word2Vec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"container\">\n",
    "  <table class=\"table\", style=\"width:80%\" >\n",
    "    <thead>\n",
    "      <tr>\n",
    "        <th style=\"text-align: center\">Word2Vec</th>\n",
    "        <th style=\"text-align: center\">BERT</th>\n",
    "      </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "      <tr>\n",
    "        <td style=\"text-align: left\">No establece relación de contexto o relación entre palabras de una oración.</td>\n",
    "         <td style=\"text-align: left\">Establece un contexto, la habilidad de bidireccionalidad le permite establecer un contexto de relación entre palabras y oraciones</td>\n",
    "      </tr>  \n",
    "         <tr>\n",
    "        <td style=\"text-align: left\">Embeddings no toman en cuenta el posicionamiento de la palabra</td>\n",
    "         <td style=\"text-align: left\">La posición de cada palabra en una frase es tomada en cuenta como entrada antes de calcular el embebido</td>\n",
    "      </tr>  \n",
    "                 <tr>\n",
    "        <td style=\"text-align: left\">La entrada para el modelo es una sola palabra</td>\n",
    "         <td style=\"text-align: left\">La entrada para el modelo son frases</td>\n",
    "      </tr>  \n",
    "    </tbody>\n",
    "    <tfoot>\n",
    "      <tr>\n",
    "        <td style=\"text-align: left\" colspan=\"2\"> Fuente: \n",
    "        </td>\n",
    "      </tr>\n",
    "    </tfoot>\n",
    "  </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca6971e",
   "metadata": {},
   "source": [
    "### Desarrollos posteriores a BERT\n",
    "\n",
    "Al ser BERT un código abierto, permite que diferentes usuarios pueden afinar la arquitectura del modelo BERT con entrenamiento supervisado para optimizarlo para su eficiencia o especializarlo para ciertas tareas, esto ha permitido que se produzcan varios desarrollos, algunos ejemplos son los siguientes:\n",
    "\n",
    "- **patentBERT**: Desarrollado por Jieh-Sheng Lee y Jieh Hsiang, es un modelo BERT ajustado para realizar la clasificación de patentes.\n",
    "\n",
    "- **docBERT**: Desarrollado por Raphael Tang y Jimmy Lin, es un modelo BERT perfeccionado para la clasificación de documentos.\n",
    "\n",
    "- **bioBERT**: Desarrollado por Jinhyuk Lee y Wonjin Yoon, es un modelo de representación de lenguaje biomédico previamente entrenado para la minería de textos biomédicos.\n",
    "\n",
    "- **VideoBERT**: Desarrollado por Chen Sun y Carl Vondrick, es un modelo visual-lingüístico conjunto para procesar el aprendizaje no supervisado de una gran cantidad de datos sin etiquetar en Youtube.\n",
    "\n",
    "- **SciBERT**: Desarrollado por Iz Beltag, Kyle Lo y Arman Cohan, es un modelo BERT previamente entrenado para texto científico.\n",
    "\n",
    "- **G-BERT**: Desarrollado por Junyuan Shang, Tengfei Ma, Cao Xiao y  Jimeng Sun, es un modelo BERT previamente entrenado usando códigos médicos con representaciones jerárquicas usando redes neuronales gráficas (GNN) y luego ajustado para hacer recomendaciones médicas.\n",
    "\n",
    "- **RoBERTa**: Desarrollado por Facebook, RoBERTa se basa en la estrategia de enmascaramiento del lenguaje de BERT y modifica algunos de los hiperparámetros clave en BERT. Para mejorar el procedimiento de entrenamiento, RoBERTa elimina la tarea de predicción de la siguiente oración (NSP) del preentrenamiento de BERT e introduce un enmascaramiento dinámico para que el token enmascarado cambie durante las épocas de entrenamiento. También se entrenó en un orden de magnitud más datos que BERT, durante un período de tiempo más largo.\n",
    "\n",
    "- **DistilBERT**: Desarrollado por HuggingFace, es un modelo de transformadores, más pequeño y más rápido que BERT, que fue entrenado previamente en el mismo corpus de una manera autosupervisada, utilizando el modelo base BERT como profesor. Esto significa que fue entrenado previamente solo en los textos en bruto, sin que los humanos los etiquetaran de ninguna manera (por lo que puede usar muchos datos disponibles públicamente) con un proceso automático para generar entradas y etiquetas a partir de esos textos utilizando el modelo base BERT.Tiene un 40 % menos de parámetros que bert-base sin recaudación, funciona un 60 % más rápido mientras preserva más del 95 % de los rendimientos de BERT medidos en el punto de referencia de comprensión del lenguaje GLUE.\n",
    "\n",
    "- **XLM-R**: Desarrollado por Facebook, es un nuevo modelo que utiliza técnicas de entrenamiento autosupervisadas para lograr un rendimiento de última generación en la comprensión interlingüe, una tarea en la que un modelo se entrena en un idioma y luego se utiliza con otros idiomas sin datos de entrenamiento adicionales. Nuestro modelo mejora los enfoques multilingües anteriores al incorporar más datos e idiomas de capacitación, incluidos los llamados lenguajes de bajos recursos, que carecen de extensos conjuntos de datos etiquetados y no etiquetados. Este modelo supera significativamente a BERT multilingüe (mBERT) en una variedad de puntos de referencia multilingües.\n",
    "\n",
    "- **ALBERT (A Lite BERT for Self-Supervised Learning of Language Representations)**: Desarrollado conjuntamente por Google Research y el Instituto Tecnológico Toyota,está preparado para ser el sucesor de BERT, presenta dos técnicas de reducción de parámetros para reducir el consumo de memoria y aumentar la velocidad de entrenamiento de BERT. Además, también tiene su propio método de entrenamiento llamado Predicción de Orden de Senunencia (SOP) que se centra en la coherencia entre oraciones y está diseñado para abordar la ineficacia de la pérdida de predicción de la siguiente oración (NSP) propuesta en el BERT original.\n",
    "\n",
    "\n",
    "Como puede observarse, se están desarrollando muchos modelos basados en BERT, esto se debe principalmente al hecho de que es un modelo de código abierto, al cual, realizando un ajuste fino, permite obtener una amplia cantidad de aplicaciones prácticas.\n",
    "\n",
    "Esto genera un gran avance en el uso del aprendizaje automático para el procesamiento del lenguaje natural. Las máquinas ahora pueden entender mejor el habla y responder de forma inteligente en tiempo real. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc642752",
   "metadata": {},
   "source": [
    "## BERT Aplicaciones \n",
    "\n",
    "Después de explicar el contexto de BERT a continuación les presentamos dos ejemplos de aplicación de BERT que pueden ser encontrados en github.\n",
    "\n",
    "1. Prediccion de palabras:  El primer ejemplo recibe como entrada una frase con una parted en blanco o masked que debe ser completada por BERT. Para ejecutar el modelo vamos a utilizar dos frases en Inglés y en Espanol para poder comparar los resultados de la predicción en ambos lenguajes para las mismas oraciones en diferentes idiomas. \n",
    "\n",
    "          Artificial Intelligence [MASK] take over the world.\n",
    "          Inteligencia Artificial [MASK] conquistar el mundo.\n",
    "\n",
    "          Coffee is the [MASK] drink in the morning\n",
    "          El cafe es la [MASK] bebida para empezar el dia\n",
    "\n",
    "2. [Análisis de Sentimiento](https://github.com/pysentimiento/pysentimiento). Este repositorio contien aplicaciones de BERT para determinar sentimiento positivo, negativo o neutral, igualmente contiene análisis de emociones tristeza, alegria, etc. Como entrada vamos a utilizar las siguiente frases: \n",
    "\n",
    "Resultado Esperado Positivo: \n",
    "\n",
    "          Hoy es un dia maravilloso  \n",
    "         \n",
    "Resultado Esperado Negativo: \n",
    "\n",
    "          Hoy es un dia gris y muy frio. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca21ee0e",
   "metadata": {},
   "source": [
    "#### Ejemplo 1 Prediccion de Palabra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d56ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Paso No. 1 Ejecutar este comando para descargar los paquetes necesarios. \n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ebc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 2 Ejecutar este comando para importar el paquete que necesitamos con la funcion que vamos \n",
    "#a utilizar para predecir \n",
    "\n",
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986e659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paso 3. Llamar la funcion unmasker para la primera frase en inglés y observar los resultados \n",
    "#El output son palabras como:  CAN, WILL, TO, WOULD \n",
    "unmasker(\"Artificial Intelligence [MASK] take over the world.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904c6c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paso 4. Llamar la funcion unmasker para la segunda frase en espanol y observar los resultados \n",
    "unmasker(\"Inteligencia Artificial [MASK] conquistar el mundo.\")\n",
    "#El output es conquistar.  \n",
    "#Podemos observar que la predicción en espanol no es tan completa como en inglés, \n",
    "#solo nos predijo una sola palabra \"conquistar\" y tambien presento algunos errores gramaticales por ejemplo\n",
    "#inteligencia artificialia conquistar el mundo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a50fbf9",
   "metadata": {},
   "source": [
    "#### Ejemplo 2 Análisis de Sentimiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e95f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paso 1 correr el comando para instalar la libreria pysentimiento. \n",
    "#La referencia al proyecto de github la pueden encontrar en la parte de arriba\n",
    "\n",
    "pip install pysentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f2bb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paso 2 Importar las librerias que necesitamos para poder utilizar las funciones de analisis de sentimiento. \n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pysentimiento/robertuito-sentiment-analysis\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"pysentimiento/robertuito-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b704ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paso 3 Importar la libreria create analyzer, despues podemos utilizar la funcion predict \n",
    "#con el input que necesitamos analizar, en este caso vamos a pasar como parametro el parametro lang=es \n",
    "#para utilizar espanol. \n",
    "\n",
    "from pysentimiento import create_analyzer\n",
    "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380dcaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4 Resultado esperado: Positivo. output = POS\n",
    "analyzer.predict(\"Hoy es un dia maravilloso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034c7f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 5 Resultado esperado: Negativo. output = NEG \n",
    "analyzer.predict(\"Hoy es un dia gris y muy frio.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4f1f6",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "A partir del trabajo presentado se puede concluir que: \n",
    "\n",
    "•\tEl modelo BERT permite resolver diferentes problemas de procesamiento de lenguaje natural, utilizando las redes neuronales Transformers como base para su funcionamiento.\n",
    "\n",
    "•\tBERT se utiliza en diferentes aplicaciones, entre ellas resumen de textos, análisis de sentimientos, traducción de textos, generación de documentos, responder preguntas, entre otros.\n",
    "\n",
    "•\tEl desarrollo de BERT ha optimizado la búsqueda en el motor de Google y ha logrado un avance significativo en lo que se refiere a la comunicación computadora-hombre, consiguiendo que los humanos se puedan comunicar utilizando un lenguaje cada vez mas natural con las máquinas.\n",
    "\n",
    "•\tAl tratarse de un código abierto y de rápido ajuste, deja abierta una gran puerta para el desarrollo de nuevos modelos basados en BERT, como patentBERT, docBERT, RoBERTa, DestilBERT, ALBERT, entre otras.\n",
    "\n",
    "•   Implementar soluciones a problemas de PLN en dentro de diferentes aplicaciones esta cada vez mas al alcance de todas las personas gracias a BERT ya que su state-of-art modelo permite a los desarrolladores tomar ventajas de sus capacidades y al ser open source existen ya modelos preentrenados listos para ser utilizados, esto reduce considerablement el tiempo de desarrollo ayudando con la reducción de costos, lo cual lo hace más al alcance de todos. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed07ed1",
   "metadata": {},
   "source": [
    "## Referencias bibliográficas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa2c613",
   "metadata": {},
   "source": [
    "- <cite data-footcite=\"Figure1\">[Paper attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)</cite>\n",
    "\n",
    "https://medium.com/swlh/differences-between-word2vec-and-bert-c08a3326b5d1#:~:text=Word2Vec%20will%20generate%20the%20same,vectors%20like%20beach%2C%20coast%20etc.\n",
    "\n",
    "https://www.blog.google/products/search/search-language-understanding-bert/\n",
    "\n",
    "https://huggingface.co/blog/bert-101?text=Paris+is+the+%5BMASK%5D+of+France.\n",
    "\n",
    "https://www.cyberclick.es/numerical-blog/google-bert-que-es-como-funciona-y-como-te-afecta\n",
    "\n",
    "RANKBRAIN\n",
    "https://www.abc.us.org/ojs/index.php/ei/article/view/539/1041\n",
    "\n",
    "Rafael Lucca, ¿Qué es BERT y cómo funciona?.\n",
    "https://dxmedia.net/algoritmo-bert-google/\n",
    "ALBERT\n",
    "https://openreview.net/pdf?id=H1eA7AEtvS\n",
    " \n",
    " \n",
    "https://towardsdatascience.com/understanding-bert-is-it-a-game-changer-in-nlp-7cca943cf3ad\n",
    " \n",
    "https://kryptonsolid.com/que-es-bert-modelo-de-lenguaje-y-como-funciona/\n",
    " \n",
    " \n",
    " \n",
    "DocBERT\n",
    "https://www.researchgate.net/publication/332493790_DocBERT_BERT_for_Document_Classification\n",
    " \n",
    " \n",
    "bioBERT\n",
    "https://arxiv.org/ftp/arxiv/papers/1901/1901.08746.pdf\n",
    " \n",
    "VideoBERT\n",
    "https://www.researchgate.net/publication/332186832_VideoBERT_A_Joint_Model_for_Video_and_Language_Representation_Learning\n",
    " \n",
    "SciBERT\n",
    "https://aclanthology.org/D19-1371.pdf\n",
    " \n",
    "G-BERT\n",
    "https://arxiv.org/abs/1906.00346\n",
    "\n",
    "\n",
    "patentBERT\n",
    "https://www.kaggle.com/datasets/danofer/patentbert\n",
    "Roberta\n",
    "https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/\n",
    "\n",
    "\n",
    "\n",
    "XLM-R \n",
    "https://ai.facebook.com/blog/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision/\n",
    "\n",
    "\n",
    "DestilBERT\n",
    "https://huggingface.co/docs/transformers/model_doc/distilbert\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "95px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.852px",
    "left": "1311px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
