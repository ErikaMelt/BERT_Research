{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b51438",
   "metadata": {},
   "source": [
    "---\n",
    "title: Hablemos de BERT Red Neuronal\n",
    "author: Erika Paola Ortiz y Romina Soledad Iglesias \n",
    "date: today\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e930b08f",
   "metadata": {},
   "source": [
    "# Hablemos de BERT Red Neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5427963c",
   "metadata": {},
   "source": [
    "## Palabras clave\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf6dad7",
   "metadata": {},
   "source": [
    "- **BERT**: Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "- **PLN:** Procesamiento de Lenguaje Natural  (NLP en Inglés): Es un campo de las ciencias de la computación, de la inteligencia artificial y de la lingüística que estudia las interacciones entre las computadoras y el lenguaje humano. Se ocupa de la formulación e investigación de mecanismos eficaces computacionalmente para la comunicación entre personas y máquinas por medio del lenguaje natural, es decir, de las lenguas del mundo.\n",
    "\n",
    "- **Deep Learning:** Aprendizaje profundo (en inglés, deep learning) es un conjunto de algoritmos de aprendizaje automático (en inglés, machine learning) que intenta modelar abstracciones de alto nivel en datos usando arquitecturas computacionales que admiten transformaciones no lineales múltiples e iterativas de datos expresados en forma matricial o tensorial. \n",
    "\n",
    "* **Machine learning:** es una disciplina, dentro de la Inteligencia Artificial, donde se estudian métodos para hacer predicciones, que podamos programar y automatizar. Machine Learning dota a las máquinas de la capacidad de aprender.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4acfcbc",
   "metadata": {},
   "source": [
    "## Introducción\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe41970a",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f3ec91",
   "metadata": {},
   "source": [
    "## Objetivos y Metodología\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83dfc95",
   "metadata": {},
   "source": [
    "- Explicar el concepto de BERT. \n",
    "- Identificar las aplicaciones del modelo BERT. \n",
    "- Presentar un demo de algunas de las aplicaciones de BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946ccb95",
   "metadata": {},
   "source": [
    "## Resultados de la Investigacion\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0712071",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ¿Qué es BERT?\n",
    "\n",
    "BERT es un modelo de machine learning de código abierto creado y publicado en 2018 por Jacob Devlin y sus compañeros en Google con el objetivo de mejorar el procesamiento de lenguage natural y ser utilizado para optimizar los resultados del motor de búsqueda de Google, BERT esta basado en el uso  de redes neuronales Transformers. El modelo BERT original se creó usando dos corpus de lengua inglesa: BookCorpus y Wikipedia en inglés.\n",
    "\n",
    "BERT por sus siglas en Inglés ***Bidirectional Encoder Representations from Transformers*** funciona de manera bidireccional, es decir, analiza los textos ya sea de derecha a izquierda como de izquierda a derecha, esto permite que no solo reconozca los términos claves de una búsqueda, sino que entienda el contexto de las palabras en la frase u oración, interpretando las búsquedas de manera más precisa. \n",
    "\n",
    "Por ejemplo: En la figura 1, se puede observar un ejemplo de cómo el algoritmo aplica la bidireccionalidad. En este caso la palabra clave **cura** presenta dos significados diferentes que dependen del contexto que la acompañe. En la primera oración lo que determina su significado se encuentra en la parte anterior, mientras que en la segunda oración el significado queda determinado por la palabra que la continua.\n",
    "\n",
    "<div>\n",
    "<img src=\"figures/ejemplo_bidireccionalidad.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "Esta nueva capacidad de BERT de bidireccionalidad permite entender mejor el lenguage natural y mejorar el resultado de búsquedas basandose no solo en Keyworkds sino utilizando todo el contexto de la frase, por ejemplo en búsquedas antes de BERT en el año 2019 un resultado para la frase: **\"2019 turista brasileño a EE.UU. necesita visa\"** arrojaba diferentes resultados antes y despues de integrar BERT en el algoritmo de búsqueda de Google como se observa en la tabla:\n",
    "\n",
    "<div class=\"container\">\n",
    "  <table class=\"table\", style=\"width:80%\">\n",
    "    <thead>\n",
    "      <tr>\n",
    "        <th>Search Text</th>\n",
    "        <th>Results before BERT 2019</th>\n",
    "        <th>Results after BERT 2019</th>\n",
    "      </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "      <tr>\n",
    "        <td>2019 turista brasileño a EE.UU. necesita visa</td>\n",
    "        <td>Información turistica para ciudadanos estadounidenses que querian visitar Brasil*1</td>\n",
    "        <td><img src=\"figures/search_results_bert.png\" width=\"800\"*2/></td>\n",
    "      </tr>     \n",
    "    </tbody>\n",
    "  </table>\n",
    "</div>\n",
    "\n",
    "*1 antes de BERT la preposion ***\"a\"*** era ignorada por el buscador generando que la frase cambiara de contexto por lo tanto los resultados no eran los esperados por el usuario. \n",
    "\n",
    "*2 con BERT aplicado al motor de búsqueda, la preposicion ***a*** es tenida en cuenta lo cual mejora la calidad de los resultados. \n",
    "\n",
    "\n",
    "Es importante destacar que BERT no sustituye por completo al anterior algoritmo, ***RankBrain*** (el primero en el que Google introdujo la inteligencia artificial), más bien lo complementa. En este sentido, BERT afecta a aquellas búsquedas más complejas, las que pueden depender del contexto o el tono.  Con Google BERT, el algoritmo podrá identificar correctamente el sentido de la búsqueda, mostrando resultados más acordes a la misma.\n",
    "\n",
    "El 25 de octubre de 2019, Google Search anunció que habían comenzado a aplicar modelos BERT para consultas de búsqueda en inglés dentro de Estados Unidos. El 9 de diciembre de 2019, se informó que BERT había sido integrado a Google Search para más de 70 idiomas.\n",
    "\n",
    "\n",
    "Por otro lado, los modelos como word2vec o GloVe generan una representación de una sola palabra para cada palabra en el vocabulario, mientras que BERT tiene en cuenta el contexto para cada aparición de una palabra determinada. Por ejemplo, mientras que el vector para palabras polisémicas como \"estrella\" tendrá la misma representación vectorial de word2vec para sus dos ocurrencias en las oraciones \"Es una estrella de rock\" y \"El sol es una estrella\", BERT proporciona una representación diferente para cada oración.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "779f4c35",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6d9932a",
   "metadata": {},
   "source": [
    "### Redes Neuronales Transformers\n",
    "\n",
    "Las redes neuronales Transformers son modelos que tienen diversas aplicaciones como traducir textos, escribir poemas, sintetizar y/o generar texto, generar código fuente entre otras aplicaciones. \n",
    " \n",
    "Las redes transformers fueron creados por Google en  2017 y por la Universidad de Toronto. En el momento de su introducción, los modelos de lenguaje usaban principalmente redes neuronales recurrentes (RNN) y redes neuronales convolucionales (CNN) para manejar tareas de Procesamiento de Lenguaje Natural (PNL).\n",
    "Estos modelos aún son competentes, sin embargo, el Transformer se considera una mejora significativa ya que no requiere que las secuencias de datos se procesen en un orden fijo, mientras que las RNN y las CNN sí lo hacen. \n",
    "\n",
    "Su enfoque principal de los Transformers era realizar traducciones, pero puede ser entrenado para diferentes usos ya que poseen una alta capacidad de procesamiento en paralelo lo que los hace más eficientes reduciendo el tiempo y facilidad de entrenamiento y aumentando el tamaño de los set de datos que pueden procesar. \n",
    " \n",
    "Los transformers están basados en tres principales innovaciones: \n",
    "- Positioning encoding\n",
    "- Attention\n",
    "- Self attention. \n",
    "\n",
    "A continuación explicamos en qué consiste cada una de las tres principales innovaciones:\n",
    " \n",
    "- **Positioning Encoding**\n",
    "\n",
    "Los transformers utilizan codificadores posicionales para etiquetar elementos de datos que entran y salen de la red. \n",
    "\n",
    "El Transformer recibe una oración de entrada y la convierte en dos secuencias: una secuencia de vectores de palabras y una secuencia de codificaciones posicionales. Ambos vectores son escritos usando representaciones numéricas del texto para que la red neuronal pueda procesarlas. Cada palabra del diccionario se representa como un vector. Las codificaciones posicionales son una representación vectorial de la posición de la palabra en la oración original.\n",
    "\n",
    "El transformer junta ambas secuencias y pasa el resultado a través de una serie de codificadores, seguidos de una serie de decodificadores. Esto es necesario debido a que el input no es alimentado en la red de forma secuencial sino que se pasa todo de una vez.\n",
    "\n",
    "Referencia: https://www.linkedin.com/pulse/transformers-redes-neuronales-cristian-santander/?originalSubdomain=es\n",
    "\n",
    " \n",
    "- **Attention**\n",
    "\n",
    "Attention es un mecanismo que permite a un modelo de texto «mirar» cada una de las palabras de la frase original al tomar una decisión sobre cómo traducir las palabras de la frase de salida.\n",
    " \n",
    "Este modelo aprende de los datos de entrenamiento. Por ejemplo, en el caso de los traductores, al ver miles de ejemplos de frases en diferentes idiomas, el modelo aprende qué tipos de palabras son interdependientes. De esta manera aprende a respetar el género, la pluralidad y otras reglas gramaticales.\n",
    "\n",
    "Referencia: https://www.ibidemgroup.com/edu/traduccion-automatica-gpt3-bert-t5/\n",
    "\n",
    " \n",
    " \n",
    "- **Self attention**\n",
    " \n",
    "El mecanismo Self attention es lo que permite al modelo saber con qué otra palabra de la oración está relacionada la palabra que se procesa en ese instante de tiempo. \n",
    "\n",
    "Es una capa que ayuda al codificador a ver otras palabras en la oración de entrada mientras codifica una palabra específica.\n",
    "\n",
    "Referencia: https://e-archivo.uc3m.es/bitstream/handle/10016/32792/TFG_Iago_Collarte_Gonzalez.pdf?sequence=1\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95571e6d",
   "metadata": {},
   "source": [
    "### Arquitectura del modelo de transformadores \n",
    "\n",
    "<div>\n",
    "<img src=\"figures/transformers-architecture.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "<cite data-footcite=\"Figure1\">Source: Figure 1: The Transformer - model architecture. Wasani et al.., 2017 [Paper attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)</cite>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed342fa6",
   "metadata": {},
   "source": [
    "## Entrenamiento de BERT\n",
    "\n",
    "Como ya mencionamos anteriormente, BERT funciona de manera bidireccional, esto permite que no solo reconozca los términos claves de una búsqueda, sino que entienda el contexto de las palabras en la frase u oración. \n",
    "Para lograr esto, el entrenamiento de BERT se basa en dos estrategias:\n",
    "\n",
    "- **Mask Language Model (MLM)**, consiste en enmascar algunas de las palabras de la entrada y luego se condiciona cada palabra bidireccionalmente para predecir las palabras enmascaradas. Para ello, al tomar una oración, el modelo enmascara al azar el 15% de las palabras en la entrada. Luego, el modelo intenta predecir el valor original de las palabras enmascaradas, basándose en el contexto proporcionado por las otras palabras no enmascaradas de la secuencia.\n",
    "\n",
    "- **Predicción de la siguiente oración (Next Sentence Prediction, NSP)**, en esta etapa, BERT aprende a modelar las relaciones entre oraciones. Para ello, en el proceso de entrenamiento, el modelo recibe pares de oraciones como entrada y aprende a predecir si la segunda oración del par es la oración posterior del documento original. Es decir, si se tienen dos oraciones A y B, BERT debe descifrar si B es la siguiente oración real que viene después de A en el corpus, o es solo una oración aleatoria.\n",
    "\n",
    "Al entrenar el modelo BERT, ambas técnicas se entrenan juntas, de esta forma, se minimiza la función de pérdida combinada de las dos estrategias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc74fd5a",
   "metadata": {},
   "source": [
    "## Aplicaciones de BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e67d4a",
   "metadata": {},
   "source": [
    "El modelo BERT puede ayudar a resolver problemas de procesamiento de lenguage natural como: \n",
    "- Traducción de textos. \n",
    "- Generación de documentos. \n",
    "- Resumen de textos. \n",
    "- Responder Preguntas.  \n",
    "- Analisis de sentimientos (por ejemplo analisis de reviews, analisis del mercado de valores en donde se analicen dos variables reviews positivas o negativas, estado  positivo o negativo del mercado de valores). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca6971e",
   "metadata": {},
   "source": [
    "## Desarrollos posteriores a BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb53e2b",
   "metadata": {},
   "source": [
    "Al ser BERT un código abierto, permite que diferentes usuarios pueden afinar la arquitectura del modelo BERT con entrenamiento supervisado para optimizarlo para su eficiencia o especializarlo para ciertas tareas, esto ha permitido que se produzcan varios desarrollos, algunos ejemplos son los siguientes:\n",
    "\n",
    "- **patentBERT**: Desarrollado por Jieh-Sheng Lee y Jieh Hsiang, es un modelo BERT ajustado para realizar la clasificación de patentes.\n",
    "\n",
    "- **docBERT**: Desarrollado por Raphael Tang y Jimmy Lin, es un modelo BERT perfeccionado para la clasificación de documentos.\n",
    "\n",
    "- **bioBERT**: Desarrollado por Jinhyuk Lee y Wonjin Yoon, es un modelo de representación de lenguaje biomédico previamente entrenado para la minería de textos biomédicos.\n",
    "\n",
    "- **VideoBERT**: Desarrollado por Chen Sun y Carl Vondrick, es un modelo visual-lingüístico conjunto para procesar el aprendizaje no supervisado de una gran cantidad de datos sin etiquetar en Youtube.\n",
    "\n",
    "- **SciBERT**: Desarrollado por Iz Beltag, Kyle Lo y Arman Cohan, es un modelo BERT previamente entrenado para texto científico.\n",
    "\n",
    "- **G-BERT**: Desarrollado por Junyuan Shang, Tengfei Ma, Cao Xiao y  Jimeng Sun, es un modelo BERT previamente entrenado usando códigos médicos con representaciones jerárquicas usando redes neuronales gráficas (GNN) y luego ajustado para hacer recomendaciones médicas.\n",
    "\n",
    "- **RoBERTa**: Desarrollado por Facebook, RoBERTa se basa en la estrategia de enmascaramiento del lenguaje de BERT y modifica algunos de los hiperparámetros clave en BERT. Para mejorar el procedimiento de entrenamiento, RoBERTa elimina la tarea de predicción de la siguiente oración (NSP) del preentrenamiento de BERT e introduce un enmascaramiento dinámico para que el token enmascarado cambie durante las épocas de entrenamiento. También se entrenó en un orden de magnitud más datos que BERT, durante un período de tiempo más largo.\n",
    "\n",
    "- **DistilBERT**: Desarrollado por HuggingFace, es un modelo de transformadores, más pequeño y más rápido que BERT, que fue entrenado previamente en el mismo corpus de una manera autosupervisada, utilizando el modelo base BERT como profesor. Esto significa que fue entrenado previamente solo en los textos en bruto, sin que los humanos los etiquetaran de ninguna manera (por lo que puede usar muchos datos disponibles públicamente) con un proceso automático para generar entradas y etiquetas a partir de esos textos utilizando el modelo base BERT.Tiene un 40 % menos de parámetros que bert-base sin recaudación, funciona un 60 % más rápido mientras preserva más del 95 % de los rendimientos de BERT medidos en el punto de referencia de comprensión del lenguaje GLUE.\n",
    "\n",
    "- **XLM-R**: Desarrollado por Facebook, es un nuevo modelo que utiliza técnicas de entrenamiento autosupervisadas para lograr un rendimiento de última generación en la comprensión interlingüe, una tarea en la que un modelo se entrena en un idioma y luego se utiliza con otros idiomas sin datos de entrenamiento adicionales. Nuestro modelo mejora los enfoques multilingües anteriores al incorporar más datos e idiomas de capacitación, incluidos los llamados lenguajes de bajos recursos, que carecen de extensos conjuntos de datos etiquetados y no etiquetados. Este modelo supera significativamente a BERT multilingüe (mBERT) en una variedad de puntos de referencia multilingües.\n",
    "\n",
    "- **ALBERT (A Lite BERT for Self-Supervised Learning of Language Representations)**: Desarrollado conjuntamente por Google Research y el Instituto Tecnológico Toyota,está preparado para ser el sucesor de BERT, presenta dos técnicas de reducción de parámetros para reducir el consumo de memoria y aumentar la velocidad de entrenamiento de BERT. Además, también tiene su propio método de entrenamiento llamado Predicción de Orden de Senunencia (SOP) que se centra en la coherencia entre oraciones y está diseñado para abordar la ineficacia de la pérdida de predicción de la siguiente oración (NSP) propuesta en el BERT original.\n",
    "\n",
    "\n",
    "Como puede observarse, se están desarrollando muchos modelos basados en BERT, esto se debe principalmente al hecho de que es un modelo de código abierto, al cual, realizando un ajuste fino, permite obtener una amplia cantidad de aplicaciones prácticas.\n",
    "\n",
    "Esto genera un gran avance en el uso del aprendizaje automático para el procesamiento del lenguaje natural. Las máquinas ahora pueden entender mejor el habla y responder de forma inteligente en tiempo real. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4f1f6",
   "metadata": {},
   "source": [
    "## Conclusiones\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa0a0be",
   "metadata": {},
   "source": [
    "- El modelo BERT ha ayudado a mejorar los diferentes problemas de procesamiento de Lenguaje Natural, gracias a su enfoque bidireccional, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed07ed1",
   "metadata": {},
   "source": [
    "## Referencias bibliográficas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa2c613",
   "metadata": {},
   "source": [
    "- <cite data-footcite=\"Figure1\">[Paper attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)</cite>\n",
    "\n",
    "https://www.blog.google/products/search/search-language-understanding-bert/\n",
    "\n",
    "https://www.cyberclick.es/numerical-blog/google-bert-que-es-como-funciona-y-como-te-afecta\n",
    "\n",
    "RANKBRAIN\n",
    "https://www.abc.us.org/ojs/index.php/ei/article/view/539/1041\n",
    "\n",
    "Rafael Lucca, ¿Qué es BERT y cómo funciona?.\n",
    "https://dxmedia.net/algoritmo-bert-google/\n",
    "ALBERT\n",
    "https://openreview.net/pdf?id=H1eA7AEtvS\n",
    " \n",
    " \n",
    "https://towardsdatascience.com/understanding-bert-is-it-a-game-changer-in-nlp-7cca943cf3ad\n",
    " \n",
    "https://kryptonsolid.com/que-es-bert-modelo-de-lenguaje-y-como-funciona/\n",
    " \n",
    " \n",
    " \n",
    "DocBERT\n",
    "https://www.researchgate.net/publication/332493790_DocBERT_BERT_for_Document_Classification\n",
    " \n",
    " \n",
    "bioBERT\n",
    "https://arxiv.org/ftp/arxiv/papers/1901/1901.08746.pdf\n",
    " \n",
    "VideoBERT\n",
    "https://www.researchgate.net/publication/332186832_VideoBERT_A_Joint_Model_for_Video_and_Language_Representation_Learning\n",
    " \n",
    "SciBERT\n",
    "https://aclanthology.org/D19-1371.pdf\n",
    " \n",
    "G-BERT\n",
    "https://arxiv.org/abs/1906.00346\n",
    "\n",
    "\n",
    "patentBERT\n",
    "https://www.kaggle.com/datasets/danofer/patentbert\n",
    "Roberta\n",
    "https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/\n",
    "\n",
    "\n",
    "\n",
    "XLM-R \n",
    "https://ai.facebook.com/blog/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision/\n",
    "\n",
    "\n",
    "DestilBERT\n",
    "https://huggingface.co/docs/transformers/model_doc/distilbert\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd0c3625",
   "metadata": {
    "tags": [
     "hide_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/MdEYUliufmk\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/MdEYUliufmk\" frameborder=\"0\" allowfullscreen></iframe>')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad93619a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "95px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.852px",
    "left": "1311px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
