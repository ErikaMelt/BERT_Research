{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b7ee43c",
   "metadata": {},
   "source": [
    "\\renewcommand\\refname{Bibliografía}\n",
    "\\renewcommand{\\contentsname}{Contenido}\n",
    "\\renewcommand{\\listfigurename}{Lista de Figuras}\n",
    "\\linespread{1.3}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fceb8d0",
   "metadata": {},
   "source": [
    "\\tableofcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fee4f86",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9b9252",
   "metadata": {},
   "source": [
    "\\listoffigures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47401b",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5427963c",
   "metadata": {},
   "source": [
    "\\section{Palabras clave}\n",
    "\n",
    "- **BERT**: Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "- **PLN:** Procesamiento de Lenguaje Natural  (NLP en Inglés): Es un campo de las ciencias de la computación, de la inteligencia artificial y de la lingüística que estudia las interacciones entre las computadoras y el lenguaje humano. Se ocupa de la formulación e investigación de mecanismos eficaces computacionalmente para la comunicación entre personas y máquinas por medio del lenguaje natural, es decir, de las lenguas del mundo.\n",
    "\n",
    "- **Deep Learning:** Aprendizaje profundo (en inglés, deep learning) es un conjunto de algoritmos de aprendizaje automático (en inglés, machine learning) que intenta modelar abstracciones de alto nivel en datos usando arquitecturas computacionales que admiten transformaciones no lineales múltiples e iterativas de datos expresados en forma matricial o tensorial. \n",
    "\n",
    "- **Machine learning:** Es una disciplina, dentro de la Inteligencia Artificial, donde se estudian métodos para hacer predicciones, que podamos programar y automatizar. Machine Learning dota a las máquinas de la capacidad de aprender.\n",
    "\n",
    "- **Search Engine Optimization:** Es un conjunto de acciones orientadas a mejorar el posicionamiento de un sitio web en la lista de resultados de Google o cualquier otro buscador de internet. ​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519361f8",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4acfcbc",
   "metadata": {},
   "source": [
    "\\section{Introducción}\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) es un modelo de machine learning de código abierto desarrollado por investigadores de Google en 2018. Se creó con el objetivo de mejorar el procesamiento de lenguaje natural y, de esta manera, lograr que el motor de búsqueda de Google interprete las consultas de una manera más precisa.\n",
    "\n",
    "Su funcionamiento está basado en las redes neuronales Transformers, desarrolladas por Google en 2017, las cuales son una mejora de los modelos de lenguaje utilizados hasta entonces.\n",
    "\n",
    "El modelo BERT puede ser utilizado en diferentes aplicaciones como traducción de textos, responder preguntas, generación de documentos, desambiguación del sentido de las palabras, entre otras, y, a partir de él, se están generando un gran número de nuevas aplicaciones.\n",
    "\n",
    "Este trabajo busca explicar el concepto de BERT, en que se basa su funcionamiento y como funciona. Se detallarán algunos de los desarrollos que se realizaron a partir de BERT y, por último, se hará una presentación de dos de las aplicaciones de BERT. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786bd7cc",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f3ec91",
   "metadata": {},
   "source": [
    "\\section{Objetivos} \n",
    "\n",
    "1. Explicar el concepto de BERT. \n",
    "2. Identificar las aplicaciones del modelo BERT. \n",
    "3. Presentar un demo de algunas de las aplicaciones de BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd39f4c",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946ccb95",
   "metadata": {},
   "source": [
    "\\section{Resultados de la Investigacion} \n",
    "\\subsection{¿Qué es BERT?} \n",
    "\n",
    "BERT es un modelo de machine learning de código abierto creado y publicado en 2018 por Jacob Devlin y otros colaboradores de Google con el objetivo de mejorar el procesamiento de lenguaje natural y ser utilizado para optimizar los resultados del motor de búsqueda. BERT está basado en el uso de redes neuronales Transformers. El modelo BERT original se creó usando dos corpus de lengua inglesa: BookCorpus y Wikipedia. \n",
    "\n",
    "BERT por sus siglas en Inglés ***Bidirectional Encoder Representations from Transformers*** funciona de manera bidireccional, es decir, analiza los textos ya sea de derecha a izquierda como de izquierda a derecha, esto permite que no solo reconozca los términos claves de una búsqueda, sino que entienda el contexto de las palabras en la frase u oración, interpretando las búsquedas de manera más precisa. \n",
    "\n",
    "Por ejemplo: En la figura 1, se puede observar un ejemplo de cómo el algoritmo aplica la bidireccionalidad. En este caso la palabra clave **cura** presenta dos significados diferentes que dependen del contexto que la acompañe. En la primera oración lo que determina su significado se encuentra en la parte anterior, mientras que en la segunda oración el significado queda determinado por la palabra que la continua.\n",
    "\n",
    "\n",
    "\\begin{figure}[hp]\n",
    "\\centering % imagen centrada\n",
    "\\captionsetup{justification=centering,margin=2cm}\n",
    "\\includegraphics[width=0.8\\textwidth]{figures/ejemplo_bidireccionalidad.png}\n",
    "\\caption{Figura 1. Ejemplo de Bidireccionalidad}\n",
    "Figura 1. Ejemplo de Bidireccionalidad\n",
    "\\end{figure}\n",
    "\n",
    "Esta nueva capacidad de BERT de bidireccionalidad permite entender mejor el lenguage natural y mejorar el resultado de búsquedas basandose no solo en las palabras claves (keywords) sino utilizando todo el contexto de la frase, por ejemplo en búsquedas antes de BERT en el año 2019 un resultado para la frase: **\"2019 turista brasileño a EE.UU. necesita visa\"** arrojaba diferentes resultados antes y despues de integrar BERT en el algoritmo de búsqueda de Google como se observa en la tabla:\n",
    "\n",
    "| Query | Search Engine Results Before and After using BERT | \n",
    "| --- | --- |\n",
    "| \"2019 brazil traveler **to** usa need a visa.”|\\includegraphics[width=.5\\textwidth]{figures/search_result_ex1.png} | \n",
    "| “do estheticians **stand** a lot at work.”|\\includegraphics[width=.5\\textwidth]{figures/search_result_ex2.png} | \n",
    "\n",
    "La palabra **\"to\"** era ignorada por el motor de búsqueda cambiando el contexto de la frase por lo tanto sin esa preposición el algoritmo arrojaba resultados de ciudadanos estadounidenses viajando a Brazil y no lo contrario que era un ciudadano brasilero viajando a los Estados Unidos. \n",
    "\n",
    "El algoritmo de búsqueda de Google tenía un enfoque basado en keywords antes de implementar BERT, en el ejemplo el texto \"stand-alone\" estaba enlazado con la palabra \"stand\" pero no lo relacionaba con el esfuerzo físico del trabajo de una esteticista, con el modelo BERT, gracias a la bidireccioonalidad y entender el contexto, esta relación es más clara por lo tanto el resultado es más ajustado a lo que el usuario desea buscar utilizando el lenguage natural. \n",
    "\n",
    "Es importante destacar que BERT no sustituye por completo al anterior algoritmo, ***RankBrain*** (el primero en el que Google introdujo la inteligencia artificial), más bien lo complementa. En este sentido, BERT afecta a aquellas búsquedas más complejas, las que pueden depender del contexto o el tono. Con Google BERT, el algoritmo podrá identificar correctamente el sentido de la búsqueda, mostrando resultados más acordes a la misma.\n",
    "\n",
    "El 25 de octubre de 2019, Google Search anunció que habían comenzado a aplicar modelos BERT para consultas de búsqueda en inglés dentro de Estados Unidos. El 9 de diciembre de 2019, se informó que BERT había sido integrado a Google Search para más de 70 idiomas.\n",
    "\n",
    "Fuente: \\cite{BERTSearch}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d9932a",
   "metadata": {},
   "source": [
    "\\subsubsection{¿Qué son las Redes Neuronales Transformers?}\n",
    "\n",
    "BERT basa su funcionamiento en las redes neuronales Transformers. Dichas redes son modelos que tienen diversas aplicaciones como traducir textos, escribir poemas, sintetizar y/o generar texto, generar código fuente entre otras aplicaciones. \n",
    " \n",
    "Las redes transformers fueron creadas por Google en  2017 y por la Universidad de Toronto. En el momento de su introducción, los modelos de lenguaje usaban principalmente redes neuronales recurrentes (RNN) y redes neuronales convolucionales (CNN) para manejar tareas de Procesamiento de Lenguaje Natural (PNL).\n",
    "Estos modelos aún son competentes, sin embargo, el Transformer se considera una mejora significativa ya que no requiere que las secuencias de datos se procesen en un orden fijo, mientras que las RNN y las CNN sí lo hacen. \n",
    "\n",
    "El enfoque principal de los Transformers era realizar traducciones, pero puede ser entrenado para diferentes usos ya que poseen una alta capacidad de procesamiento en paralelo lo que los hace más eficientes reduciendo el tiempo y facilidad de entrenamiento y aumentando el tamaño de los set de datos que pueden procesar. \n",
    " \n",
    "Los Transformers están basados en tres principales innovaciones: \n",
    "- Positioning encoding\n",
    "- Attention\n",
    "- Self attention. \n",
    "\n",
    "A continuación explicamos en qué consiste cada una de las tres principales innovaciones:\n",
    " \n",
    "- **Positioning Encoding**\n",
    "\n",
    "Los Transformers utilizan codificadores posicionales para etiquetar elementos de datos que entran y salen de la red. \n",
    "\n",
    "El Transformer recibe una oración de entrada y la convierte en dos secuencias: una secuencia de vectores de palabras y una secuencia de codificaciones posicionales. Ambos vectores son escritos usando representaciones numéricas del texto para que la red neuronal pueda procesarlas. Cada palabra del diccionario se representa como un vector. Las codificaciones posicionales son una representación vectorial de la posición de la palabra en la oración original.\n",
    "\n",
    "El Transformer junta ambas secuencias y pasa el resultado a través de una serie de codificadores, seguidos de una serie de decodificadores. Esto es necesario debido a que el input no es alimentado en la red de forma secuencial sino que se pasa todo de una vez.\n",
    "\n",
    "Fuente: \\cite{vaswani2017attention}\n",
    "\n",
    "- **Attention**\n",
    "\n",
    "Attention es un mecanismo que permite a un modelo de texto «mirar» cada una de las palabras de la frase original al tomar una decisión sobre cómo traducir las palabras de la frase de salida.\n",
    " \n",
    "Este modelo aprende de los datos de entrenamiento. Por ejemplo, en el caso de los traductores, al ver miles de ejemplos de frases en diferentes idiomas, el modelo aprende qué tipos de palabras son interdependientes. De esta manera aprende a respetar el género, la pluralidad y otras reglas gramaticales.\n",
    "\n",
    "Fuente: \\cite{Transformers1}\n",
    " \n",
    "- **Self attention**\n",
    " \n",
    "El mecanismo Self attention es lo que permite al modelo saber con que otra palabra de la oración está relacionada la palabra que se procesa en ese instante de tiempo. \n",
    "\n",
    "Es una capa que ayuda al codificador a ver otras palabras en la oración de entrada mientras codifica una palabra específica.\n",
    "\n",
    "Fuente: \\cite{BERTModel}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95571e6d",
   "metadata": {},
   "source": [
    "\\subsubsection{Arquitectura del modelo de transformadores}\n",
    "\n",
    "\\begin{figure}\n",
    "\\centering % imagen centrada\n",
    "\\captionsetup{justification=centering,margin=2cm}\n",
    "\\includegraphics[width=.5\\textwidth]{figures/transformers-architecture.png} \n",
    "\\caption{Figura 2. Transformers Architecture}\n",
    "Figura 2. Transformers Architecture. Fuente: \\cite{vaswani2017attention}.\n",
    "\\end{figure}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed342fa6",
   "metadata": {},
   "source": [
    "\\subsection{¿Cómo funciona BERT?}\n",
    "\n",
    "Como ya mencionamos anteriormente, BERT esta basado en **Transformers**, el mecanismo de atención que permite que se comprenda el contexto y se entienda mejor las relaciones entre palabras dentro de una frase u oración. \n",
    "\n",
    "\n",
    "\\begin{figure}\n",
    "\\centering % imagen centrada\n",
    "\\captionsetup{justification=centering,margin=2cm}\n",
    "\\includegraphics[width=.5\\textwidth]{figures/bert_working.png} \n",
    "\\caption{Figura 3. BERT Working}\n",
    "Figura 3. BERT Working. Fuente: \\cite{BERTExplained}\n",
    "\\end{figure}\n",
    "\n",
    "En la imagen observamos dos frases que van a ser utilizadas como entrada en BERT, por lo tanto podemos observar que una secuencia de tokens es convertida en vectores para posteriormente ser procesada en la red neuronal. A continuación explicamos que significa cada etapa. \n",
    "\n",
    "1. **Token embeddings:** El token [CLS] se agrega al inicio de la primera frase y el token [SEP] se agrega al final de cada frase u oración. (Ver Figura 3) \n",
    "2. **Segment embeddings:** Se agrega un indicador que identifique la frase A y la frase B, Esta identificación permite que el codificador distinga las dos frases A y B. \n",
    "3. **Positional embeddings:** Un indicador de posición es agregado a cada token para indicar su posición en la frase. \n",
    "\n",
    "\n",
    "\\subsubsection{Fases de preentrenamiento de BERT}\n",
    "\n",
    "A continuación explicamos las fases de pre-entrenamiento del modelo BERT: \n",
    "\n",
    "- **Dataset de entrenamiento**\n",
    "\n",
    "BERT fue entrenado con un dataset de más de 3.3 billones de palabras tomados de Wikipedia (~2.5B words) y Google BooksCorpus (~800M words), esto ha contribuido a su inmenso éxito para entender el lenguage natural. \n",
    "\n",
    "Procesar esa cantidad de datos requiere tener un gran poder computacional, por lo tanto BERT tomó ventaja de la arquitectura de transformers y  la velocidad de procesamiento de las TPUs (Tensor Processing Units, Google construyó este circuito exclusivamente para procesamiento de modelos de ML). \n",
    "\n",
    "Para entrenar BERT se utilizaron 64 TPUs durante 4 días. \n",
    "\n",
    "- **Mask Language Model (MLM)**\n",
    "\n",
    "Consiste en enmascar algunas de las palabras de la entrada y luego se condiciona cada palabra bidireccionalmente para predecir las palabras enmascaradas. Para ello, al tomar una oración, el modelo enmascara al azar el 15% de las palabras en la entrada. Luego, el modelo intenta predecir el valor original de las palabras enmascaradas, basándose en el contexto proporcionado por las otras palabras no enmascaradas de la secuencia. Por ejemplo:  \n",
    "\n",
    "\\begin{verbatim}\n",
    "    - “Dang! I’m out fishing and a huge trout just [MASK] my line!\"\n",
    "\\end{verbatim}\n",
    "\n",
    "El modelo predice la palabra que iría en el MASK dependiendo del contexto obtenido de la frase, en este caso podría decir algo como **Broke**. \n",
    "\n",
    "Los humanos por naturaleza utilizamos el masking, es parte natural del uso del lenguage, para nosotros como humanos esta tarea es trivial pero lograr que una máquina logre o tenga la capacidad de realizar esta tarea es parte de lo que el modelo BERT está logrando.  \n",
    "    \n",
    "\n",
    "- **Predicción de la siguiente oración (Next Sentence Prediction, NSP)**\n",
    "\n",
    "En esta etapa, BERT aprende a modelar las relaciones entre oraciones. Para ello, en el proceso de entrenamiento, el modelo recibe pares de oraciones como entrada y aprende a predecir si la segunda oración del par es la oración posterior del documento original. Es decir, si se tienen dos oraciones A y B, BERT debe descifrar si B es la siguiente oración real que viene después de A en el corpus, o es solo una oración aleatoria. Por ejemplo: \n",
    "\n",
    "\\begin{verbatim}\n",
    "    - Erika fue al supermercado. Ella compró un delicioso postre. (Correct NSP)\n",
    "    - Romina preparó café. El helado de vainilla es el mejor. (Incorrect NSP)\n",
    "\\end{verbatim}\n",
    "\n",
    "Al entrenar el modelo BERT, ambas técnicas se entrenan juntas, de esta forma, se minimiza la función de pérdida combinada de las dos estrategias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146739ca",
   "metadata": {},
   "source": [
    "\\subsection{Modelos antecesores a BERT}\n",
    "\n",
    "Previo a BERT se han desarrollado varios modelos de procesamiento de lenguaje, entre ellos podemos mencionar:\n",
    "\n",
    "**Word2Vec**: Fue creado en 2013 por Google. Este modelo fue uno de los primeros en ser escalable pudiendo generar incrustaciones de palabras para grandes corpus. El inconveniente de este modelo es que toma una sola palabra como entrada y produce una sola representación vectorial de esa palabra, sin tener en cuenta el contexto, lo cual crea inconvenientes cuando se trata de palabras polisémicas. Fuente: \\cite{BERTDifferences}\n",
    "\n",
    "Por ejemplo, en el caso de la palabra **consejo** tendrá el mismo vector asociado cuando se use en las siguientes frases:\n",
    "\n",
    "\\begin{verbatim}\n",
    "    - El sabio le dio un **consejo** y él supo escucharlo.\n",
    "    - El **consejo** de administración da por concluida la reunión.\n",
    "\\end{verbatim}\n",
    "\n",
    "\n",
    "**Glo Ve (Global Vectors for Word Representation)**: Fue creado como un proyecto de código abierto en la Universidad de Stanford. Este modelo genera solo un vector (incrustación) para cada palabra, combinando todos los diferentes sentidos de la palabra en un solo vector, por lo que no tiene en cuenta en qué contexto se está utilizando la palabra, al igual que Word2Vec. Fuente: \\cite{bautista2019analisis}\n",
    "\n",
    "\n",
    "**ElMo (Embeddings from language Models)**: Fue creado AllenNLP. Al igual que BERT, ELMo se calcula utilizando un modelo de lenguaje bidireccional para producir una representación vectorial. Por lo tanto, las incrustaciones ElMo son capaces de capturar el contexto de la palabra utilizada en la oración y pueden generar diferentes incrustaciones para la misma palabra utilizada en un contexto diferente en diferentes oraciones.\n",
    "A pesar de esto, el inconveniente que tiene ElMo es que, si bien se trata de un modelo bidireccional, sólo concatena la información de izquierda a derecha y de derecha a izquierda, pero no puede aprovechar los contextos de izquierda y derecha simultáneamente, que es el gran avance que se ha logrado con BERT.  Fuente: \\cite{ElmoBert}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc74fd5a",
   "metadata": {},
   "source": [
    "\\subsection{Aplicaciones de BERT}\n",
    "\n",
    "El modelo BERT puede ayudar a resolver problemas de procesamiento de lenguaje natural como:\n",
    "\n",
    "- ***Análisis de Sentimientos:*** Puede analizar y determinar si una critica es positiva o negativa, clasificar el email entre spam y no spam.\n",
    "\n",
    "- ***Responder Preguntas:*** Por ejemplo chatbox que responden preguntas. \n",
    "\n",
    "- ***Traducción de textos:*** Por ejemplo google translate utiliza BERT para realizar la traducción de textos. \n",
    "\n",
    "- ***Predicción de texto:*** Por ejemplo Gmail puede predecir el siguiente texto cuando estas redactando un email. \n",
    "\n",
    "- ***Resumen de textos:*** Google search engine utiliza BERT y en los resultados de búsqueda cuando te aparece un resumen de un resultado específico es también parte de lo que BERT puede hacer. \n",
    "\n",
    "- ***Generación de textos:*** Puede escribir un artículo acerca de cualquier tema con solo unas pocas frases como entrada.\n",
    "\n",
    "- ***Resolución de polisemia y correferencia:*** Diferenciar entre palabras que suenan o se ven iguales pero tienen diferentes significados, por ejemplo la palabra **Banco** que dependiendo del contexto de la oración puede tomar diferentes significados, aquí algunos ejemplos: \n",
    "\n",
    "\\begin{verbatim}\n",
    "    - ¡No te sientes en ese **banco** que está roto! \n",
    "    - \"El **banco** central de mi país subió las tasas de intereses de todas sus operaciones\"\n",
    "\\end{verbatim}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca6971e",
   "metadata": {},
   "source": [
    "\\subsection{Desarrollos posteriores a BERT}\n",
    "\n",
    "Al ser BERT un código abierto, permite que diferentes usuarios pueden afinar la arquitectura del modelo BERT con entrenamiento supervisado para optimizarlo para su eficiencia o especializarlo para ciertas tareas, esto ha permitido que se produzcan varios desarrollos, algunos ejemplos son los siguientes:\n",
    "\n",
    "- **patentBERT**: Desarrollado por Jieh-Sheng Lee y Jieh Hsiang, es un modelo BERT ajustado para realizar la clasificación de patentes. Fuente: \\cite{patentBERT}\n",
    "\n",
    "- **docBERT**: Desarrollado por Raphael Tang y Jimmy Lin, es un modelo BERT perfeccionado para la clasificación de documentos. Fuente: \\cite{adhikari2019docbert}\n",
    "\n",
    "- **bioBERT**: Desarrollado por Jinhyuk Lee y Wonjin Yoon, es un modelo de representación de lenguaje biomédico previamente entrenado para la minería de textos biomédicos. Fuente: \\cite{lee2020biobert}\n",
    "\n",
    "- **VideoBERT**: Desarrollado por Chen Sun y Carl Vondrick, es un modelo visual-lingüístico conjunto para procesar el aprendizaje no supervisado de una gran cantidad de datos sin etiquetar en Youtube. Fuente: \\cite{sun2019videobert}\n",
    "\n",
    "- **SciBERT**: Desarrollado por Iz Beltag, Kyle Lo y Arman Cohan, es un modelo BERT previamente entrenado para texto científico. Fuente: \\cite{beltagy2019scibert}\n",
    "\n",
    "- **G-BERT**: Desarrollado por Junyuan Shang, Tengfei Ma, Cao Xiao y  Jimeng Sun, es un modelo BERT previamente entrenado usando códigos médicos con representaciones jerárquicas usando redes neuronales gráficas (GNN) y luego ajustado para hacer recomendaciones médicas. Fuente: \\cite{shang2019pre}\n",
    "\n",
    "- **RoBERTa**: Desarrollado por Facebook, RoBERTa se basa en la estrategia de enmascaramiento del lenguaje de BERT y modifica algunos de los hiperparámetros clave en BERT. Para mejorar el procedimiento de entrenamiento, RoBERTa elimina la tarea de predicción de la siguiente oración (NSP) del preentrenamiento de BERT e introduce un enmascaramiento dinámico para que el token enmascarado cambie durante las épocas de entrenamiento. También se entrenó en un orden de magnitud más de datos que BERT, durante un período de tiempo más largo. Fuente: \\cite{liu2019roberta}\n",
    "\n",
    "- **DistilBERT**: Desarrollado por HuggingFace, es un modelo de transformadores, más pequeño y más rápido que BERT, que fue entrenado previamente en el mismo corpus de una manera autosupervisada, utilizando el modelo base BERT como profesor. Esto significa que fue entrenado previamente solo en los textos en bruto, sin que los humanos los etiquetaran de ninguna manera (por lo que puede usar muchos datos disponibles públicamente) con un proceso automático para generar entradas y etiquetas a partir de esos textos utilizando el modelo base BERT. Tiene un 40 % menos de parámetros que BERT-base sin recaudación, funciona un 60 % más rápido mientras preserva más del 95 % de los rendimientos de BERT medidos en el punto de referencia de comprensión del lenguaje GLUE. Fuente: \\cite{DistilBERT}\n",
    "\n",
    "- **XLM-R**: Desarrollado por Facebook, es un nuevo modelo que utiliza técnicas de entrenamiento autosupervisadas para lograr un rendimiento de última generación en la comprensión interlingüe, una tarea en la que un modelo se entrena en un idioma y luego se utiliza con otros idiomas sin datos de entrenamiento adicionales. Este modelo mejora los enfoques multilingües anteriores al incorporar más datos e idiomas de capacitación, incluidos los llamados lenguajes de bajos recursos, que carecen de extensos conjuntos de datos etiquetados y no etiquetados. Este modelo supera significativamente a BERT multilingüe (mBERT) en una variedad de puntos de referencia multilingües. Fuente: \\cite{conneau2019unsupervised}\n",
    "\n",
    "- **ALBERT (A Lite BERT for Self-Supervised Learning of Language Representations)**: Desarrollado conjuntamente por Google Research y el Instituto Tecnológico Toyota,está preparado para ser el sucesor de BERT, presenta dos técnicas de reducción de parámetros para reducir el consumo de memoria y aumentar la velocidad de entrenamiento de BERT. Además, también tiene su propio método de entrenamiento llamado Predicción de Orden de Senunencia (SOP) que se centra en la coherencia entre oraciones y está diseñado para abordar la ineficacia de la pérdida de predicción de la siguiente oración (NSP) propuesta en el BERT original. Fuente: \\cite{lan2019albert}\n",
    "\n",
    "\n",
    "Como puede observarse, se están desarrollando muchos modelos basados en BERT, esto se debe principalmente al hecho de que es un modelo de código abierto, al cual, realizando un ajuste fino, permite obtener una amplia cantidad de aplicaciones prácticas.\n",
    "\n",
    "Esto genera un gran avance en el uso del aprendizaje automático para el procesamiento del lenguaje natural. Las máquinas ahora pueden entender mejor el habla y responder de forma inteligente en tiempo real. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49216a3d",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970214be",
   "metadata": {},
   "source": [
    "\\subsection{¿Cuál es el impacto de BERT en SEO?}\n",
    "\n",
    "Google anunció el lanzamiento de BERT a nivel internacional en Diciembre 2019,  sin embargo el impacto en SEO no fue tan notorio en comparación con actualizaciones anteriores, esto debido a que las herramientas de tracking que se usan para medir los cambios en SEO generalmente hacen tracking de short keywords, y en este caso la implementación de BERT mejora los resultados de búsqueda a partir de un mejor entendimiento del lenguaje natural y no basandose únicamente en el uso de keywords.  \n",
    "\n",
    "Antes de BERT, los creadores de contenido web, enfocaban la optimización de sus sitios web usando keywords con el fin de rankear su contenido y generar tráfico, esto no siempre es ventajoso ya que el tráfico que se genera de esta manera no necesariamente es de calidad. \n",
    "\n",
    "El uso de keywords para optimizar y posicionar un sitio web, en algunos casos tiende a deshumanizar el contenido web, sin embargo a partir de la inclusión de BERT, se busca una mayor naturalidad y sencillez al momento de redactar los textos, lo que trae aparejado un tráfico de mayor calidad en la web ya que al interpretar mejor la búsqueda del usuario, lo redirigirá a la web en cuestión. \n",
    "\n",
    "Las paginas web deben seguir creando contenido de calidad enfocado en el usuario, sin crear textos enfocados a ser entendidos por los robots de Google, esta tendencia puede generar una disminución el tráfico recibido que se puede traducir en un aumento de las visitas de calidad.  \n",
    "\n",
    "En conclusión la actualización de BERT, no genera un impacto negativo, por lo contrario mejora las visitas que se reciben en el sitio web, siempre y cuando el contenido web sea de calidad y este enfocado en sus usuarios objetivos.   Si el contenido esta bien escrito se puede llegar a mejorar los indicadores de conversión y reducción de churn.\n",
    "\n",
    "Fuente: /cite{SEOBert}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcad384",
   "metadata": {},
   "source": [
    "\\section{Ejemplos de implementación de BERT} \n",
    "\n",
    "Después de explicar el contexto de BERT a continuación les presentamos dos ejemplos de aplicaciones de BERT: \n",
    "\n",
    "1. **Prediccion de palabras (Fill Mask):**  El primer ejemplo recibe como entrada una frase con una parted en blanco o masked que debe ser completada por BERT. Para ejecutar el modelo vamos a utilizar dos frases en Inglés y en Español para poder comparar los resultados de la predicción en ambos lenguajes.  \n",
    "\n",
    "\n",
    "\\begin{figure}\n",
    "\\centering % imagen centrada\n",
    "\\captionsetup{justification=centering,margin=2cm}\n",
    "\\includegraphics[width=.5\\textwidth]{figures/fill-mask.png} \n",
    "\\caption{Figura 4. Fill Mask Process}\n",
    "Figura 4. Fill Mask Process. Fuente: \\cite{FillMask}.\n",
    "\\end{figure}\n",
    "\n",
    "\n",
    "2. **Análisis de Sentimiento.**: Fuente: \\cite{perez2021pysentimiento}. Esta libreria de Python se enfoca en análisis de sentimiento, actualmente soporta:   \n",
    "\n",
    "- Sentiment Analysis (Spanish, English)\n",
    "- Emotion Analysis (Spanish, English)\n",
    "- Hate Speech Detection (Spanish, English)\n",
    "- Named Entity Recognition (Spanish + English)\n",
    "- POS Tagging (Spanish + English)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e36386",
   "metadata": {},
   "source": [
    "\\subsection{Ejemplo 1 Prediccion de Palabra (Fill-Mask)} "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10482e8",
   "metadata": {},
   "source": [
    "**Paso 1**\n",
    "\n",
    "Ejecutar este comando para descargar los paquetes necesarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a59713",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4731a305",
   "metadata": {},
   "source": [
    "**Paso 2**\n",
    "\n",
    "Ejecutar este comando para importar el paquete que necesitamos con la funcion que vamos a utilizar para predecir el output de la palabra que esta masked.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d0773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac49b99d",
   "metadata": {},
   "source": [
    "**Paso 3** \n",
    "\n",
    "Llamar la función unmasker y pasar la frase con la palabra MASKED que queremos predecir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9bb388e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.3182411193847656,\n",
       "  'token': 2064,\n",
       "  'token_str': 'can',\n",
       "  'sequence': 'artificial intelligence can take over the world.'},\n",
       " {'score': 0.1829962283372879,\n",
       "  'token': 2097,\n",
       "  'token_str': 'will',\n",
       "  'sequence': 'artificial intelligence will take over the world.'},\n",
       " {'score': 0.05600151792168617,\n",
       "  'token': 2000,\n",
       "  'token_str': 'to',\n",
       "  'sequence': 'artificial intelligence to take over the world.'},\n",
       " {'score': 0.04519502446055412,\n",
       "  'token': 2015,\n",
       "  'token_str': '##s',\n",
       "  'sequence': 'artificial intelligences take over the world.'},\n",
       " {'score': 0.04515310749411583,\n",
       "  'token': 2052,\n",
       "  'token_str': 'would',\n",
       "  'sequence': 'artificial intelligence would take over the world.'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"Artificial Intelligence [MASK] take over the world.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7f7e34",
   "metadata": {},
   "source": [
    "**Paso 4**\n",
    "\n",
    "Llamar la función unmasker para la misma frase pero en Español y observar los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "576ae996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2017439901828766,\n",
       "  'token': 2050,\n",
       "  'token_str': '##a',\n",
       "  'sequence': 'inteligencia artificiala conquistar el mundo.'},\n",
       " {'score': 0.16341248154640198,\n",
       "  'token': 27893,\n",
       "  'token_str': '##idad',\n",
       "  'sequence': 'inteligencia artificialidad conquistar el mundo.'},\n",
       " {'score': 0.12584763765335083,\n",
       "  'token': 1061,\n",
       "  'token_str': 'y',\n",
       "  'sequence': 'inteligencia artificial y conquistar el mundo.'},\n",
       " {'score': 0.10195942968130112,\n",
       "  'token': 2721,\n",
       "  'token_str': '##la',\n",
       "  'sequence': 'inteligencia artificialla conquistar el mundo.'},\n",
       " {'score': 0.054612696170806885,\n",
       "  'token': 2401,\n",
       "  'token_str': '##ia',\n",
       "  'sequence': 'inteligencia artificialia conquistar el mundo.'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker(\"Inteligencia Artificial [MASK] conquistar el mundo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd0fb62",
   "metadata": {},
   "source": [
    "Podemos observar que la palabra que el algoritmo genero para la misma frase en Español fue Conquistar pero en Inglés, el algoritmo generó muchas más palabras como CAN, TAKE, WILL TAKE, WOULD TAKE, esto nos permite concluir que el modelo en Inglés tiene mejor predicción del contexto, esto puede ser debido a los datos de entrenamiento que se usaron en Inglés son mucho más grandes que en Español. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3336f0",
   "metadata": {},
   "source": [
    "\\subsection{Ejemplo 2 Análisis de Sentimiento}\n",
    "\n",
    "En este ejemplo vamos a usar dos frases en Español que expresan sentimiento positivo y negativo, con el objetivo de verificar que el algoritmo nos detecta el sentimiento correcto para cada frase. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aecd7a3",
   "metadata": {},
   "source": [
    "**Paso 1**\n",
    "\n",
    "Correr el comando para instalar la libreria pysentimiento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f70984",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pysentimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847b2b53",
   "metadata": {},
   "source": [
    "**Paso 2**\n",
    "\n",
    "Importar la función create_analyzer, que nos permitirá pasar como parámetro una tarea como: \n",
    "- sentiment = Permite predecir el sentimiento ya sea positivo o negativo de una frase. \n",
    "- emotion = Permite predecir la emoción que se expresa en una frase, por ejemplo rabia, alegria, tristeza, felicidad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d7acc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysentimiento import create_analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41e1402",
   "metadata": {},
   "source": [
    "**Paso 3**\n",
    "\n",
    "Llamar la funcion create_analyzer pasando como parámetro task=\"sentiment\" y lang=\"es\",  esto nos crea un objeto analyzer el cual tiene la función predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53e40d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c0f43e",
   "metadata": {},
   "source": [
    "**Paso 4**\n",
    "\n",
    "Pasar como parámetro la frase:  Hoy es un día maravilloso, el output esperado es POS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b423b8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnalyzerOutput(output=POS, probas={POS: 0.998, NEU: 0.001, NEG: 0.001})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.predict(\"Hoy es un dia maravilloso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73bb284",
   "metadata": {},
   "source": [
    "**Paso 5**\n",
    "\n",
    "Pasar como parámetro la frase:  Hoy es un día gris y muy frío, el output esperado es NEG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d35e4dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnalyzerOutput(output=NEG, probas={NEG: 0.997, NEU: 0.003, POS: 0.000})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.predict(\"Hoy es un dia gris y muy frio.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed71e0bd",
   "metadata": {},
   "source": [
    "\\subsection{Ejemplo 3 Análisis de Emociones}\n",
    "\n",
    "En este ejemplo vamos a utilizar el análisis de emociones para predecir la emoción que se expresa en dos frases en Inglés, una frase expresa Alegria y la otra frase expresa tristeza. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343b148b",
   "metadata": {},
   "source": [
    "**Paso 1** \n",
    "\n",
    "Llamar la funcion create_analyzer pasando como parámetro la task=\"emotion\" y lang=\"en\", esto nos inicializará un objeto emotion_analyzer,  el cuál expone el método predict.  A este método le pasamos como parámetro la frase que queremos predecir y el nos devuelve como output el nombre de la emoción que predijo de acuerdo con la frase enviada como input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3b79f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_analyzer = create_analyzer(task=\"emotion\", lang=\"en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e994288",
   "metadata": {},
   "source": [
    "**Paso 2**\n",
    "\n",
    "Llamar la funcion predict y pasar como parámetro la frase \"I'm over the moon\" que expresa emoción alegria y observar el output.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac7b89d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnalyzerOutput(output=joy, probas={joy: 0.972, others: 0.019, surprise: 0.003, sadness: 0.002, anger: 0.001, fear: 0.001, disgust: 0.001})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_analyzer.predict(\"I'm over the moon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6302996",
   "metadata": {},
   "source": [
    "**Paso 3**\n",
    "\n",
    "Llamar la funcion predict y pasar como parámetro la frase \"She is feeling blue\" que expresa emoción de tristeza y observar el output.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c1db851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnalyzerOutput(output=others, probas={others: 0.936, joy: 0.034, sadness: 0.015, fear: 0.006, surprise: 0.004, disgust: 0.003, anger: 0.002})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_analyzer.predict(\"She is feeling blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b71aa36",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4f1f6",
   "metadata": {},
   "source": [
    "\\section{Conclusiones}\n",
    "\n",
    "A partir del trabajo presentado se puede concluir que: \n",
    "\n",
    "•\tEl modelo BERT permite resolver diferentes problemas de procesamiento de lenguaje natural, utilizando las redes neuronales Transformers como base para su funcionamiento.\n",
    "\n",
    "•\tBERT se utiliza en diferentes aplicaciones, entre ellas resumen de textos, análisis de sentimientos, traducción de textos, generación de documentos, responder preguntas, entre otros.\n",
    "\n",
    "•\tEl desarrollo de BERT ha optimizado la búsqueda en el motor de Google y ha logrado un avance significativo en lo que se refiere a la comunicación computadora-hombre, consiguiendo que los humanos se puedan comunicar utilizando un lenguaje cada vez más natural con las máquinas.\n",
    "\n",
    "•\tAl tratarse de un código abierto y de rápido ajuste, deja abierta una gran puerta para el desarrollo de nuevos modelos basados en BERT, como patentBERT, docBERT, RoBERTa, DestilBERT, ALBERT, entre otras.\n",
    "\n",
    "•   Implementar soluciones a problemas de PLN, hoy en día está más al alcance de todos los desarrolladores gracias a la existencia de proyectos open source que usan BERT tomando ventajas de todas las capacidades que estos modelos pre-entrenados ofrecen, reduciendo considerablemente el tiempo de desarrollo y el time-to-market. \n",
    "\n",
    "•   BERT tiene un impacto en Search Engine Optimization (SEO), ya que BERT ha sido implementado en los algoritmos de búsqueda, por lo tanto los expertos en SEO recomiendan que el contenido no se debe basar solamente en keywords, sino generar contenido de calidad haciendo mejor uso del lenguaje natural para tomar ventajas de los avances realizados con BERT. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1488c5f3",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed07ed1",
   "metadata": {},
   "source": [
    "\\bibliographystyle{apalike}\n",
    "\\bibliography{references}"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Erika Paola Ortiz"
   },
   {
    "name": "Romina Iglesias"
   }
  ],
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "title": "Hablemos de BERT Red Neuronal",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "95px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.852px",
    "left": "1311px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
