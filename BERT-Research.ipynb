{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a3cf7c",
   "metadata": {},
   "source": [
    "\\begin{center}\n",
    "\\includegraphics[width=.400\\textwidth]{figures/bert_logo.jpeg}\n",
    "\\end{center}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578fbcff",
   "metadata": {},
   "source": [
    "\\begin{document}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576fd6b6",
   "metadata": {},
   "source": [
    "\\tableofcontents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a34c9ea",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce93ed13",
   "metadata": {},
   "source": [
    "\\listoffigures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d77bc",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5427963c",
   "metadata": {},
   "source": [
    "## Palabras clave\n",
    "\n",
    "- **BERT**: Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "- **PLN:** Procesamiento de Lenguaje Natural  (NLP en Inglés): Es un campo de las ciencias de la computación, de la inteligencia artificial y de la lingüística que estudia las interacciones entre las computadoras y el lenguaje humano. Se ocupa de la formulación e investigación de mecanismos eficaces computacionalmente para la comunicación entre personas y máquinas por medio del lenguaje natural, es decir, de las lenguas del mundo.\n",
    "\n",
    "- **Deep Learning:** Aprendizaje profundo (en inglés, deep learning) es un conjunto de algoritmos de aprendizaje automático (en inglés, machine learning) que intenta modelar abstracciones de alto nivel en datos usando arquitecturas computacionales que admiten transformaciones no lineales múltiples e iterativas de datos expresados en forma matricial o tensorial. \n",
    "\n",
    "- **Machine learning:** es una disciplina, dentro de la Inteligencia Artificial, donde se estudian métodos para hacer predicciones, que podamos programar y automatizar. Machine Learning dota a las máquinas de la capacidad de aprender.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7711b960",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4acfcbc",
   "metadata": {},
   "source": [
    "## Introducción\t\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) es un modelo de machine learning de código abierto desarrollado por investigadores de Google en 2018. Se creó con el objetivo de mejorar el procesamiento de lenguaje natural y, de esta manera, lograr que el motor de búsqueda de Google interprete las consultas de una manera más precisa.\n",
    "\n",
    "Su funcionamiento está basado en las redes neuronales Transformers, desarrolladas por Google en 2017, las cuales son una mejora de los modelos de lenguaje utilizados hasta entonces.\n",
    "\n",
    "El modelo BERT puede ser utilizado en diferentes aplicaciones como traducción de textos, responder preguntas, generación de documentos, desambiguación del sentido de las palabras, entre otras. y, a partir de él, se están generando un gran número de nuevas aplicaciones.\n",
    "\n",
    "Este trabajo busca explicar el concepto de BERT, en que se basa su funcionamiento y como funciona. Se detallarán algunos de los desarrollos que se realizaron a partir de BERT y, por último, se hará una presentación de dos de las aplicaciones de BERT. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71382a22",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f3ec91",
   "metadata": {},
   "source": [
    "## Objetivos \n",
    "\n",
    "1. Explicar el concepto de BERT. \n",
    "2. Identificar las aplicaciones del modelo BERT. \n",
    "3. Presentar un demo de algunas de las aplicaciones de BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946ccb95",
   "metadata": {},
   "source": [
    "## Resultados de la Investigacion\n",
    "\n",
    "### ¿Qué es BERT?\n",
    "\n",
    "BERT es un modelo de machine learning de código abierto creado y publicado en 2018 por Jacob Devlin y otros colaboradores de Google con el objetivo de mejorar el procesamiento de lenguage natural y ser utilizado para optimizar los resultados del motor de búsqueda, BERT esta basado en el uso de redes neuronales Transformers. El modelo BERT original se creó usando dos corpus de lengua inglesa: BookCorpus y Wikipedia. \n",
    "\n",
    "BERT por sus siglas en Inglés ***Bidirectional Encoder Representations from Transformers*** funciona de manera bidireccional, es decir, analiza los textos ya sea de derecha a izquierda como de izquierda a derecha, esto permite que no solo reconozca los términos claves de una búsqueda, sino que entienda el contexto de las palabras en la frase u oración, interpretando las búsquedas de manera más precisa. \n",
    "\n",
    "Por ejemplo: En la figura 1, se puede observar un ejemplo de cómo el algoritmo aplica la bidireccionalidad. En este caso la palabra clave **cura** presenta dos significados diferentes que dependen del contexto que la acompañe. En la primera oración lo que determina su significado se encuentra en la parte anterior, mientras que en la segunda oración el significado queda determinado por la palabra que la continua.\n",
    "\n",
    "\\begin{center}\n",
    "\\begin{figure}\n",
    "Ejemplo Bidireccionalidad\n",
    "\\includegraphics[width=\\linewidth]{figures/ejemplo_bidireccionalidad.png}\n",
    "\\caption{Fig. 1 Ejemplo de Bidireccionalidad}\n",
    "\\end{figure}\n",
    "\\end{center}\n",
    "\n",
    "Esta nueva capacidad de BERT de bidireccionalidad permite entender mejor el lenguage natural y mejorar el resultado de búsquedas basandose no solo en Keyworkds sino utilizando todo el contexto de la frase, por ejemplo en búsquedas antes de BERT en el año 2019 un resultado para la frase: **\"2019 turista brasileño a EE.UU. necesita visa\"** arrojaba diferentes resultados antes y despues de integrar BERT en el algoritmo de búsqueda de Google como se observa en la tabla:\n",
    "\n",
    "| Query | Search Engine Results Before and After using BERT | \n",
    "| --- | --- |\n",
    "| \"2019 brazil traveler **to** usa need a visa.”|\\includegraphics[width=.5\\linewidth]{figures/search_result_ex1.png} | \n",
    "| “do estheticians **stand** a lot at work.”|\\includegraphics[width=.5\\linewidth]{figures/search_result_ex2.png} | \n",
    "\n",
    "La palabra **\"to\"** era ignorada por el motor de búsqueda cambiando el contexto de la frase por lo tanto sin esa preposición el algoritmo arrojaba resultados de ciudadanos estadounidenses viajando a Brazil y no lo contrario que era un ciudadano brasilero viajando a los Estados Unidos. \n",
    "\n",
    "El algoritmo de búsqueda de Google tenia un enfoque basado en keywords antes de implementar BERT, en el ejemplo el texto \"stand-alone\" estaba enlazado con la palabra \"stand\" pero no lo relacionaba con el esfuerzo fisico del trabajo de una esteticista, con el modelo BERT gracias a la bidireccioonalidad y entender el contexto esta relacion es más clara por lo tanto el resultado es más ajustado a lo que el usuario utilizando el lenguage natural desea buscar. \n",
    "\n",
    "Es importante destacar que BERT no sustituye por completo al anterior algoritmo, ***RankBrain*** (el primero en el que Google introdujo la inteligencia artificial), más bien lo complementa. En este sentido, BERT afecta a aquellas búsquedas más complejas, las que pueden depender del contexto o el tono.  Con Google BERT, el algoritmo podrá identificar correctamente el sentido de la búsqueda, mostrando resultados más acordes a la misma.\n",
    "\n",
    "El 25 de octubre de 2019, Google Search anunció que habían comenzado a aplicar modelos BERT para consultas de búsqueda en inglés dentro de Estados Unidos. El 9 de diciembre de 2019, se informó que BERT había sido integrado a Google Search para más de 70 idiomas.\n",
    "\n",
    "Fuente: \\cite{BERTSearch} .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d9932a",
   "metadata": {},
   "source": [
    "### Redes Neuronales Transformers\n",
    "\n",
    "BERT basa su funcionamiento en las redes neuronales Transformers. Dichas redes son modelos que tienen diversas aplicaciones como traducir textos, escribir poemas, sintetizar y/o generar texto, generar código fuente entre otras aplicaciones. \n",
    " \n",
    "Las redes transformers fueron creados por Google en  2017 y por la Universidad de Toronto. En el momento de su introducción, los modelos de lenguaje usaban principalmente redes neuronales recurrentes (RNN) y redes neuronales convolucionales (CNN) para manejar tareas de Procesamiento de Lenguaje Natural (PNL).\n",
    "Estos modelos aún son competentes, sin embargo, el Transformer se considera una mejora significativa ya que no requiere que las secuencias de datos se procesen en un orden fijo, mientras que las RNN y las CNN sí lo hacen. \n",
    "\n",
    "Su enfoque principal de los Transformers era realizar traducciones, pero puede ser entrenado para diferentes usos ya que poseen una alta capacidad de procesamiento en paralelo lo que los hace más eficientes reduciendo el tiempo y facilidad de entrenamiento y aumentando el tamaño de los set de datos que pueden procesar. \n",
    " \n",
    "Los Transformers están basados en tres principales innovaciones: \n",
    "- Positioning encoding\n",
    "- Attention\n",
    "- Self attention. \n",
    "\n",
    "A continuación explicamos en qué consiste cada una de las tres principales innovaciones:\n",
    " \n",
    "- **Positioning Encoding**\n",
    "\n",
    "Los Transformers utilizan codificadores posicionales para etiquetar elementos de datos que entran y salen de la red. \n",
    "\n",
    "El Transformer recibe una oración de entrada y la convierte en dos secuencias: una secuencia de vectores de palabras y una secuencia de codificaciones posicionales. Ambos vectores son escritos usando representaciones numéricas del texto para que la red neuronal pueda procesarlas. Cada palabra del diccionario se representa como un vector. Las codificaciones posicionales son una representación vectorial de la posición de la palabra en la oración original.\n",
    "\n",
    "El Transformer junta ambas secuencias y pasa el resultado a través de una serie de codificadores, seguidos de una serie de decodificadores. Esto es necesario debido a que el input no es alimentado en la red de forma secuencial sino que se pasa todo de una vez.\n",
    "\n",
    "Fuente: \\cite{Transformers}\n",
    "\n",
    "- **Attention**\n",
    "\n",
    "Attention es un mecanismo que permite a un modelo de texto «mirar» cada una de las palabras de la frase original al tomar una decisión sobre cómo traducir las palabras de la frase de salida.\n",
    " \n",
    "Este modelo aprende de los datos de entrenamiento. Por ejemplo, en el caso de los traductores, al ver miles de ejemplos de frases en diferentes idiomas, el modelo aprende qué tipos de palabras son interdependientes. De esta manera aprende a respetar el género, la pluralidad y otras reglas gramaticales.\n",
    "\n",
    "Fuente: \\cite{Transformers1}\n",
    " \n",
    "- **Self attention**\n",
    " \n",
    "El mecanismo Self attention es lo que permite al modelo saber con qué otra palabra de la oración está relacionada la palabra que se procesa en ese instante de tiempo. \n",
    "\n",
    "Es una capa que ayuda al codificador a ver otras palabras en la oración de entrada mientras codifica una palabra específica.\n",
    "\n",
    "Fuente: \\cite{BERTModel}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95571e6d",
   "metadata": {},
   "source": [
    "### Arquitectura del modelo de transformadores \n",
    "\n",
    "\\begin{figure}\n",
    "Fig.2 Transformers Architecture \n",
    "\\includegraphics[width=.3\\textwidth]{figures/transformers-architecture.png} \n",
    "\\caption{Fig. 2 Transformers Architecture}\n",
    "Fuente: \\cite{vaswani2017attention}.\n",
    "\\end{figure}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef223be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed342fa6",
   "metadata": {},
   "source": [
    "### Cómo funciona BERT?\n",
    "\n",
    "Como ya mencionamos anteriormente, BERT esta basado en **Transformers**, el mecanismo de atención que permite que se comprenda el contexto y se entienda mejor las relaciones entre palabras dentro de una frase u oración. \n",
    "\n",
    "\\begin{center}\n",
    "\\includegraphics[width=.3\\textwidth]{figures/bert_working.png} \n",
    "\\end{center}\n",
    "\n",
    "\n",
    "<cite data-footcite=\"Figure1\">Figura: BERT Explained Wasani et al.., 2017 [BERT Explained a complete guide](https://medium.com/@samia.khalid/bert-explained-a-complete-guide-with-theory-and-tutorial-3ac9ebc8fa7c)</cite>.\n",
    "\n",
    "En la imagen observamos dos frases que van a a ser utilizadas como entrada en BERT, por lo tanto podemos observar que una secuencia de tokens es convertida en vectores para posteriormente ser procesada en la red neuronal. A continuación explicamos que significa cada etapa. \n",
    "\n",
    "1. **Token embeddings:** El token [CLS] se agrega al inicio de la primera frase y el token [SEP] se agrega al final de cada frase u oración. (ver figura adjunta) \n",
    "2. **Segment embeddings:** Se agrega un indicador que identifique la frase A y la frase B, Esta identificación permite que el codificador distinga las dos frases A y B. \n",
    "3. **Positional embeddings:** Un indicador de posición es agregado a cada token para indicar su posición en la frase. \n",
    "\n",
    "\n",
    "#### Fases de preentrenamiento de BERT**\n",
    "\n",
    "A continuación explicamos las fases de pre-entrenamiento del modelo BERT: \n",
    "\n",
    "- **Dataset de entrenamiento**\n",
    "\n",
    "BERT fue entrenado con un dataset de mas de 3.3 billones de palabras tomados de Wikipedia (~2.5B words) y Google BooksCorpus (~800M words), esto ha contribuido a su inmenso éxito para entender el lenguage natural. \n",
    "\n",
    "Procesar esa cantidad de datos requiere tener un gran poder computacional, por lo tanto BERT tomo ventaja de la arquitectura de tansformers y  la velocidad de procesamiento de las TPUs (Tensor Processing Units, Google construyó este circuito exclusivamente para procesamiento de modelos de ML). \n",
    "\n",
    "Para entrenar BERT se utilizaroon 64 TPUs durante 4 dias. \n",
    "\n",
    "- **Mask Language Model (MLM)**\n",
    "Consiste en enmascar algunas de las palabras de la entrada y luego se condiciona cada palabra bidireccionalmente para predecir las palabras enmascaradas. Para ello, al tomar una oración, el modelo enmascara al azar el 15% de las palabras en la entrada. Luego, el modelo intenta predecir el valor original de las palabras enmascaradas, basándose en el contexto proporcionado por las otras palabras no enmascaradas de la secuencia. Por ejemplo:  \n",
    "\n",
    "      - “Dang! I’m out fishing and a huge trout just [MASK] my line!\", el modelo predice la palabra que iria en el MASK dependiendo del contexto obtenido de la frase en este caso podria decir algo como **Broke**. \n",
    "\n",
    "Los humanos por naturaleza utilizamos el masking es parte natural del uso del lenguage, para nosotros como humanos esta tarea es trivial pero lograr que una máquina logre o tenga la capacidad de realizar esta tarea es parte de lo que el modelo BERT está logrando.  \n",
    "    \n",
    "\n",
    "- **Predicción de la siguiente oración (Next Sentence Prediction, NSP)**\n",
    "En esta etapa, BERT aprende a modelar las relaciones entre oraciones. Para ello, en el proceso de entrenamiento, el modelo recibe pares de oraciones como entrada y aprende a predecir si la segunda oración del par es la oración posterior del documento original. Es decir, si se tienen dos oraciones A y B, BERT debe descifrar si B es la siguiente oración real que viene después de A en el corpus, o es solo una oración aleatoria. Por ejemplo: \n",
    "\n",
    "\n",
    "     - Erika fue al supermercado. Ella compró un delicioso postre. (Correct NSP)\n",
    "     - Romina preparó café. El helado de vainilla es el mejor. (Incorrect NSP)\n",
    "\n",
    "Al entrenar el modelo BERT, ambas técnicas se entrenan juntas, de esta forma, se minimiza la función de pérdida combinada de las dos estrategias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146739ca",
   "metadata": {},
   "source": [
    "### Modelos antecesores a BERT\n",
    "\n",
    "Previo a BERT se han desarrollado varios modelos de procesamiento de lenguaje, entre ellos podemos mencionar:\n",
    "\n",
    "**Word2Vec**: fue creado en 2013 por Google. Este modelo fue uno de los primeros en ser escalable pudiendo generar incrustaciones de palabras para grandes corpus. El inconveniente de este modelo es que toma una sola palabra como entrada y produce una sola representación vectorial de esa palabra, sin tener en cuenta el contexto, lo cual crea inconvenientes cuando se trata de palabras polisémicas.\n",
    "Por ejemplo, en el caso de la palabra “**consejo**” tendrá el mismo vector asociado cuando se use en las siguientes frases:\n",
    "\n",
    "    * El sabio le dio un **consejo** y él supo escucharlo.\n",
    "    * El **consejo** de administración da por concluida la reunión.\n",
    "\n",
    "\n",
    "**GloVe (Global Vectors for Word Representation)**: fue creado como un proyecto de código abierto en la Universidad de Stanford. Este modelo genera solo un vector (incrustación) para cada palabra, combinando todos los diferentes sentidos de la palabra en un solo vector, por lo que no tiene en cuenta en qué contexto se está utilizando la palabra, al igual que Word2Vec.\n",
    "\n",
    "\n",
    "**ElMo (Embeddings from language Models)**: fue creado AllenNLP. Al igual que BERT, ELMo se calcula utilizando un modelo de lenguaje bidireccional para producir representación vectorial. Por lo tanto, las incrustaciones ELMo son capaces de capturar el contexto de la palabra utilizada en la oración y pueden generar diferentes incrustaciones para la misma palabra utilizada en un contexto diferente en diferentes oraciones.\n",
    "A pesar de esto, el inconveniente que tiene ElMo es que, si bien se trata de un modelo bidireccional, sólo concatena la información de izquierda a derecha y de derecha a izquierda, pero no puede aprovechar los contextos de izquierda y derecha simultáneamente, que es el gran avance que se ha logrado con BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc74fd5a",
   "metadata": {},
   "source": [
    "### Aplicaciones de BERT\n",
    "\n",
    "El modelo BERT puede ayudar a resolver problemas de procesamiento de lenguage natural como: \n",
    "\n",
    "- ***Análisis de Sentimientos:*** Puede analizar y determinar si una critica es positiva o negativa, clasificar el email entre spam y no spam.\n",
    "\n",
    "- ***Responder Preguntas:*** Por ejemplo chatbox que responden preguntas. \n",
    "\n",
    "- ***Traducción de textos:*** Por ejemplo google translate utiliza BERT para realizar la traducción de textos. \n",
    "\n",
    "- ***Predicción de texto:*** Por ejemplo Gmail puede predecir el siguiente texto cuando estas redactando un email. \n",
    "\n",
    "- ***Resumen de textos:*** Google search engine utiliza BERT y en los resultados de búsquda cuando te aparece un resumen de un resultado especifico es también parte de lo que BERT puede hacer. \n",
    "\n",
    "- ***Generación de textos:*** Puede escribir un articulo acerca de cualquier tema con solo unas pocas frases como entrada. \n",
    "\n",
    "- ***Resolución de polisemia y correferencia:*** Diferenciar entre palabras que suenan o se ven iguales pero tienen diferentes significados, por ejemplo la palabra **Banco** que dependediendo del contexto de la oración puede tomar diferentes significados, aqui algunos ejemplos: \n",
    "                    \n",
    "            ¡No te sientes en ese **banco** que está roto! \n",
    "            \"El **banco** central de mi país subió las tasas de intereses de todas sus operaciones\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca6971e",
   "metadata": {},
   "source": [
    "### Desarrollos posteriores a BERT\n",
    "\n",
    "Al ser BERT un código abierto, permite que diferentes usuarios pueden afinar la arquitectura del modelo BERT con entrenamiento supervisado para optimizarlo para su eficiencia o especializarlo para ciertas tareas, esto ha permitido que se produzcan varios desarrollos, algunos ejemplos son los siguientes:\n",
    "\n",
    "- **patentBERT**: Desarrollado por Jieh-Sheng Lee y Jieh Hsiang, es un modelo BERT ajustado para realizar la clasificación de patentes.\n",
    "\n",
    "- **docBERT**: Desarrollado por Raphael Tang y Jimmy Lin, es un modelo BERT perfeccionado para la clasificación de documentos.\n",
    "\n",
    "- **bioBERT**: Desarrollado por Jinhyuk Lee y Wonjin Yoon, es un modelo de representación de lenguaje biomédico previamente entrenado para la minería de textos biomédicos.\n",
    "\n",
    "- **VideoBERT**: Desarrollado por Chen Sun y Carl Vondrick, es un modelo visual-lingüístico conjunto para procesar el aprendizaje no supervisado de una gran cantidad de datos sin etiquetar en Youtube.\n",
    "\n",
    "- **SciBERT**: Desarrollado por Iz Beltag, Kyle Lo y Arman Cohan, es un modelo BERT previamente entrenado para texto científico.\n",
    "\n",
    "- **G-BERT**: Desarrollado por Junyuan Shang, Tengfei Ma, Cao Xiao y  Jimeng Sun, es un modelo BERT previamente entrenado usando códigos médicos con representaciones jerárquicas usando redes neuronales gráficas (GNN) y luego ajustado para hacer recomendaciones médicas.\n",
    "\n",
    "- **RoBERTa**: Desarrollado por Facebook, RoBERTa se basa en la estrategia de enmascaramiento del lenguaje de BERT y modifica algunos de los hiperparámetros clave en BERT. Para mejorar el procedimiento de entrenamiento, RoBERTa elimina la tarea de predicción de la siguiente oración (NSP) del preentrenamiento de BERT e introduce un enmascaramiento dinámico para que el token enmascarado cambie durante las épocas de entrenamiento. También se entrenó en un orden de magnitud más datos que BERT, durante un período de tiempo más largo.\n",
    "\n",
    "- **DistilBERT**: Desarrollado por HuggingFace, es un modelo de transformadores, más pequeño y más rápido que BERT, que fue entrenado previamente en el mismo corpus de una manera autosupervisada, utilizando el modelo base BERT como profesor. Esto significa que fue entrenado previamente solo en los textos en bruto, sin que los humanos los etiquetaran de ninguna manera (por lo que puede usar muchos datos disponibles públicamente) con un proceso automático para generar entradas y etiquetas a partir de esos textos utilizando el modelo base BERT.Tiene un 40 % menos de parámetros que bert-base sin recaudación, funciona un 60 % más rápido mientras preserva más del 95 % de los rendimientos de BERT medidos en el punto de referencia de comprensión del lenguaje GLUE.\n",
    "\n",
    "- **XLM-R**: Desarrollado por Facebook, es un nuevo modelo que utiliza técnicas de entrenamiento autosupervisadas para lograr un rendimiento de última generación en la comprensión interlingüe, una tarea en la que un modelo se entrena en un idioma y luego se utiliza con otros idiomas sin datos de entrenamiento adicionales. Nuestro modelo mejora los enfoques multilingües anteriores al incorporar más datos e idiomas de capacitación, incluidos los llamados lenguajes de bajos recursos, que carecen de extensos conjuntos de datos etiquetados y no etiquetados. Este modelo supera significativamente a BERT multilingüe (mBERT) en una variedad de puntos de referencia multilingües.\n",
    "\n",
    "- **ALBERT (A Lite BERT for Self-Supervised Learning of Language Representations)**: Desarrollado conjuntamente por Google Research y el Instituto Tecnológico Toyota,está preparado para ser el sucesor de BERT, presenta dos técnicas de reducción de parámetros para reducir el consumo de memoria y aumentar la velocidad de entrenamiento de BERT. Además, también tiene su propio método de entrenamiento llamado Predicción de Orden de Senunencia (SOP) que se centra en la coherencia entre oraciones y está diseñado para abordar la ineficacia de la pérdida de predicción de la siguiente oración (NSP) propuesta en el BERT original.\n",
    "\n",
    "\n",
    "Como puede observarse, se están desarrollando muchos modelos basados en BERT, esto se debe principalmente al hecho de que es un modelo de código abierto, al cual, realizando un ajuste fino, permite obtener una amplia cantidad de aplicaciones prácticas.\n",
    "\n",
    "Esto genera un gran avance en el uso del aprendizaje automático para el procesamiento del lenguaje natural. Las máquinas ahora pueden entender mejor el habla y responder de forma inteligente en tiempo real. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382ef865",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcad384",
   "metadata": {},
   "source": [
    "## BERT Ejemplos \n",
    "\n",
    "Después de explicar el contexto de BERT a continuación les presentamos dos ejemplos de aplicación de BERT que pueden ser encontrados en la siguiente fuente = \\cite{perez2021pysentimiento}. \n",
    "\n",
    "1. **Prediccion de palabras (Fill Mask):**  El primer ejemplo recibe como entrada una frase con una parted en blanco o masked que debe ser completada por BERT. Para ejecutar el modelo vamos a utilizar dos frases en Inglés y en Español para poder comparar los resultados de la predicción en ambos lenguajes. \n",
    "\n",
    "\n",
    "\\begin{center}\n",
    "\\includegraphics[width=.6\\textwidth]{figures/fill-mask.png}\n",
    "\\end{center}\n",
    "\n",
    "          1. Artificial Intelligence [MASK] take over the world.\n",
    "          2. Inteligencia Artificial [MASK] conquistar el mundo.\n",
    "          3. Coffee is the [MASK] drink in the morning\n",
    "          4. El cafe es la [MASK] bebida para empezar el dia\n",
    "         \n",
    "\n",
    "2. **[Análisis de Sentimiento](https://pypi.org/project/pysentimiento/).**: Esta libreria de Python se enfoca en análisis de sentimiento, actualmente soporta: \n",
    "\n",
    "- Sentiment Analysis (Spanish, English)\n",
    "- Emotion Analysis (Spanish, English)\n",
    "- Hate Speech Detection (Spanish, English)\n",
    "- Named Entity Recognition (Spanish + English)\n",
    "- POS Tagging (Spanish + English)\n",
    "\n",
    "Para el ejmploo vamos a utilizar dos frases positivas y negativas en Inglés y en Español\n",
    "\n",
    "Resultado Esperado Positivo: \n",
    "\n",
    "          1. Hoy es un dia maravilloso  \n",
    "         \n",
    "Resultado Esperado Negativo: \n",
    "\n",
    "          2. Hoy es un dia gris y muy frio. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e36386",
   "metadata": {},
   "source": [
    "#### Ejemplo 1 Prediccion de Palabra (Fill-Mask) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10482e8",
   "metadata": {},
   "source": [
    "**Paso No. 1**\n",
    "\n",
    "Ejecutar este comando para descargar los paquetes necesarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65a59713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/anaconda3/envs/uic/lib/python3.10/site-packages (4.25.1)\n",
      "Requirement already satisfied: filelock in /usr/local/anaconda3/envs/uic/lib/python3.10/site-packages (from transformers) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/anaconda3/envs/uic/lib/python3.10/site-packages (from transformers) (1.23.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/anaconda3/envs/uic/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/anaconda3/envs/uic/lib/python3.10/site-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/anaconda3/envs/uic/lib/python3.10/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /usr/local/anaconda3/envs/uic/lib/python3.10/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/anaconda3/envs/uic/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/anaconda3/envs/uic/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/anaconda3/envs/uic/lib/python3.10/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/anaconda3/envs/uic/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/anaconda3/envs/uic/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/anaconda3/envs/uic/lib/python3.10/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/anaconda3/envs/uic/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/envs/uic/lib/python3.10/site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/envs/uic/lib/python3.10/site-packages (from requests->transformers) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4731a305",
   "metadata": {},
   "source": [
    "##### Paso 2 Ejecutar este comando para importar el paquete que necesitamos con la funcion que vamos  a utilizar para predecir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5d0773d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.16398738324642181,\n",
       "  'token': 115,\n",
       "  'token_str': ' could',\n",
       "  'sequence': 'Artificial Intelligence could take over the world.'},\n",
       " {'score': 0.14330103993415833,\n",
       "  'token': 40,\n",
       "  'token_str': ' will',\n",
       "  'sequence': 'Artificial Intelligence will take over the world.'},\n",
       " {'score': 0.07904805988073349,\n",
       "  'token': 64,\n",
       "  'token_str': ' can',\n",
       "  'sequence': 'Artificial Intelligence can take over the world.'},\n",
       " {'score': 0.049935925751924515,\n",
       "  'token': 24854,\n",
       "  'token_str': ' Agents',\n",
       "  'sequence': 'Artificial Intelligence Agents take over the world.'},\n",
       " {'score': 0.03829808533191681,\n",
       "  'token': 189,\n",
       "  'token_str': ' may',\n",
       "  'sequence': 'Artificial Intelligence may take over the world.'}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased')\n",
    "unmasker1 = pipeline ('fill-mask', model='distilroberta-base')\n",
    "unmasker(\"Artificial Intelligence [MASK] take over the world.\")\n",
    "unmasker1(\"Artificial Intelligence <mask> take over the world.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9bb388e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.3182411193847656,\n",
       "  'token': 2064,\n",
       "  'token_str': 'can',\n",
       "  'sequence': 'artificial intelligence can take over the world.'},\n",
       " {'score': 0.1829962283372879,\n",
       "  'token': 2097,\n",
       "  'token_str': 'will',\n",
       "  'sequence': 'artificial intelligence will take over the world.'},\n",
       " {'score': 0.05600151792168617,\n",
       "  'token': 2000,\n",
       "  'token_str': 'to',\n",
       "  'sequence': 'artificial intelligence to take over the world.'},\n",
       " {'score': 0.04519502446055412,\n",
       "  'token': 2015,\n",
       "  'token_str': '##s',\n",
       "  'sequence': 'artificial intelligences take over the world.'},\n",
       " {'score': 0.04515310749411583,\n",
       "  'token': 2052,\n",
       "  'token_str': 'would',\n",
       "  'sequence': 'artificial intelligence would take over the world.'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paso 3. Llamar la funcion unmasker para la primera frase en inglés y observar los resultados \n",
    "# El output son palabras como:  CAN, WILL, TO, WOULD \n",
    "unmasker(\"Artificial Intelligence [MASK] take over the world.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7f7e34",
   "metadata": {},
   "source": [
    "#### Paso 4. Llamar la funcion unmasker para la segunda frase en espanol y observar los resultados \n",
    "El output es conquistar.  \n",
    "Podemos observar que la predicción en espanol no es tan completa como en inglés, solo nos predijo una sola palabra \"conquistar\" y tambien presento algunos errores gramaticales por ejemplo inteligencia artificialia conquistar el mundo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "576ae996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.2017439901828766,\n",
       "  'token': 2050,\n",
       "  'token_str': '##a',\n",
       "  'sequence': 'inteligencia artificiala conquistar el mundo.'},\n",
       " {'score': 0.16341248154640198,\n",
       "  'token': 27893,\n",
       "  'token_str': '##idad',\n",
       "  'sequence': 'inteligencia artificialidad conquistar el mundo.'},\n",
       " {'score': 0.12584763765335083,\n",
       "  'token': 1061,\n",
       "  'token_str': 'y',\n",
       "  'sequence': 'inteligencia artificial y conquistar el mundo.'},\n",
       " {'score': 0.10195942968130112,\n",
       "  'token': 2721,\n",
       "  'token_str': '##la',\n",
       "  'sequence': 'inteligencia artificialla conquistar el mundo.'},\n",
       " {'score': 0.054612696170806885,\n",
       "  'token': 2401,\n",
       "  'token_str': '##ia',\n",
       "  'sequence': 'inteligencia artificialia conquistar el mundo.'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "unmasker(\"Inteligencia Artificial [MASK] conquistar el mundo.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3336f0",
   "metadata": {},
   "source": [
    "#### Ejemplo 2 Análisis de Sentimiento "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aecd7a3",
   "metadata": {},
   "source": [
    "##### Paso 1 correr el comando para instalar la libreria pysentimiento. \n",
    "La referencia al proyecto de github la pueden encontrar en la parte de arriba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07f70984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pysentimiento\n",
      "  Using cached pysentimiento-0.5.2-py3-none-any.whl (30 kB)\n",
      "Collecting emoji<2.0.0,>=1.6.1\n",
      "  Using cached emoji-1.7.0.tar.gz (175 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/anaconda3/lib/python3.9/site-packages (from pysentimiento) (1.12.1)\n",
      "Requirement already satisfied: transformers>=4.13.0 in /usr/local/anaconda3/lib/python3.9/site-packages (from pysentimiento) (4.24.0)\n",
      "Requirement already satisfied: datasets>=1.13.3 in /usr/local/anaconda3/lib/python3.9/site-packages (from pysentimiento) (2.7.1)\n",
      "Requirement already satisfied: packaging in /usr/local/anaconda3/lib/python3.9/site-packages (from datasets>=1.13.3->pysentimiento) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/anaconda3/lib/python3.9/site-packages (from datasets>=1.13.3->pysentimiento) (6.0)\n",
      "Requirement already satisfied: dill<0.3.7 in /usr/local/anaconda3/lib/python3.9/site-packages (from datasets>=1.13.3->pysentimiento) (0.3.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/anaconda3/lib/python3.9/site-packages (from datasets>=1.13.3->pysentimiento) (2022.7.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/anaconda3/lib/python3.9/site-packages (from datasets>=1.13.3->pysentimiento) (8.0.0)\n",
      "Requirement already satisfied: pandas in /usr/local/anaconda3/lib/python3.9/site-packages (from datasets>=1.13.3->pysentimiento) (1.4.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/anaconda3/lib/python3.9/site-packages (from datasets>=1.13.3->pysentimiento) (4.64.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/anaconda3/lib/python3.9/site-packages (from datasets>=1.13.3->pysentimiento) (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/anaconda3/lib/python3.9/site-packages (from datasets>=1.13.3->pysentimiento) (1.21.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/anaconda3/lib/python3.9/site-packages (from datasets>=1.13.3->pysentimiento) (0.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/anaconda3/lib/python3.9/site-packages (from datasets>=1.13.3->pysentimiento) (2.28.1)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/anaconda3/lib/python3.9/site-packages (from datasets>=1.13.3->pysentimiento) (0.18.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/anaconda3/lib/python3.9/site-packages (from datasets>=1.13.3->pysentimiento) (0.70.12.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/anaconda3/lib/python3.9/site-packages (from datasets>=1.13.3->pysentimiento) (3.8.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/anaconda3/lib/python3.9/site-packages (from transformers>=4.13.0->pysentimiento) (2022.7.9)\n",
      "Requirement already satisfied: filelock in /usr/local/anaconda3/lib/python3.9/site-packages (from transformers>=4.13.0->pysentimiento) (3.6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/anaconda3/lib/python3.9/site-packages (from transformers>=4.13.0->pysentimiento) (0.13.1)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/anaconda3/lib/python3.9/site-packages (from torch->pysentimiento) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/anaconda3/lib/python3.9/site-packages (from packaging->datasets>=1.13.3->pysentimiento) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.13.3->pysentimiento) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.13.3->pysentimiento) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.13.3->pysentimiento) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets>=1.13.3->pysentimiento) (2.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (1.8.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets>=1.13.3->pysentimiento) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/anaconda3/lib/python3.9/site-packages (from pandas->datasets>=1.13.3->pysentimiento) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/anaconda3/lib/python3.9/site-packages (from pandas->datasets>=1.13.3->pysentimiento) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets>=1.13.3->pysentimiento) (1.16.0)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171032 sha256=803e2afad0c43830190e716a90ade428c946059e304d77576e76f36f48b860d3\n",
      "  Stored in directory: /Users/eo/Library/Caches/pip/wheels/fa/7a/e9/22dd0515e1bad255e51663ee513a2fa839c95934c5fc301090\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji, pysentimiento\n",
      "Successfully installed emoji-1.7.0 pysentimiento-0.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pysentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d7acc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysentimiento import create_analyzer\n",
    "analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b423b8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnalyzerOutput(output=POS, probas={POS: 0.998, NEU: 0.001, NEG: 0.001})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.predict(\"Hoy es un dia maravilloso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d35e4dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnalyzerOutput(output=NEG, probas={NEG: 0.997, NEU: 0.003, POS: 0.000})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.predict(\"Hoy es un dia gris y muy frio.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f409657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Análisis de Emociones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b3b79f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72eb11b28c56469b81ef56ab2857f6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/999 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/eo/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"others\",\n",
      "    \"1\": \"joy\",\n",
      "    \"2\": \"sadness\",\n",
      "    \"3\": \"anger\",\n",
      "    \"4\": \"surprise\",\n",
      "    \"5\": \"disgust\",\n",
      "    \"6\": \"fear\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 3,\n",
      "    \"disgust\": 5,\n",
      "    \"fear\": 6,\n",
      "    \"joy\": 1,\n",
      "    \"others\": 0,\n",
      "    \"sadness\": 2,\n",
      "    \"surprise\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93b3c235eb24a9c87c16b246a431c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/540M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file pytorch_model.bin from cache at /Users/eo/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at finiteautomata/bertweet-base-emotion-analysis.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb656bd531141c69893b3360747a3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/295 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/eo/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"others\",\n",
      "    \"1\": \"joy\",\n",
      "    \"2\": \"sadness\",\n",
      "    \"3\": \"anger\",\n",
      "    \"4\": \"surprise\",\n",
      "    \"5\": \"disgust\",\n",
      "    \"6\": \"fear\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 3,\n",
      "    \"disgust\": 5,\n",
      "    \"fear\": 6,\n",
      "    \"joy\": 1,\n",
      "    \"others\": 0,\n",
      "    \"sadness\": 2,\n",
      "    \"surprise\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94bcf3e035704367a7c9b986f9e236da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/843k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373c70e1cfa740728eec5d35ee114043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142b2ce994e34251ab40c13c5813e6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/17.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1efd486c93f42c083ee69b50b3a9306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at /Users/eo/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/vocab.txt\n",
      "loading file bpe.codes from cache at /Users/eo/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/bpe.codes\n",
      "loading file added_tokens.json from cache at /Users/eo/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/added_tokens.json\n",
      "loading file special_tokens_map.json from cache at /Users/eo/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /Users/eo/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/eo/.cache/huggingface/hub/models--finiteautomata--bertweet-base-emotion-analysis/snapshots/64046df9cc41eab40e1ecde7d2b7fb42b971be5b/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"finiteautomata/bertweet-base-emotion-analysis\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"others\",\n",
      "    \"1\": \"joy\",\n",
      "    \"2\": \"sadness\",\n",
      "    \"3\": \"anger\",\n",
      "    \"4\": \"surprise\",\n",
      "    \"5\": \"disgust\",\n",
      "    \"6\": \"fear\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"anger\": 3,\n",
      "    \"disgust\": 5,\n",
      "    \"fear\": 6,\n",
      "    \"joy\": 1,\n",
      "    \"others\": 0,\n",
      "    \"sadness\": 2,\n",
      "    \"surprise\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 130,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 64001\n",
      "}\n",
      "\n",
      "Adding <mask> to the vocabulary\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnalyzerOutput(output=anger, probas={anger: 0.972, others: 0.007, disgust: 0.006, joy: 0.005, surprise: 0.004, fear: 0.003, sadness: 0.003})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_analyzer = create_analyzer(task=\"emotion\", lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac7b89d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnalyzerOutput(output=joy, probas={joy: 0.879, others: 0.106, surprise: 0.005, anger: 0.005, sadness: 0.002, disgust: 0.002, fear: 0.002})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_analyzer.predict(\"yayyy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c1db851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnalyzerOutput(output=anger, probas={anger: 0.972, others: 0.007, disgust: 0.006, joy: 0.005, surprise: 0.004, fear: 0.003, sadness: 0.003})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_analyzer.predict(\"fuck off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4f1f6",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "A partir del trabajo presentado se puede concluir que: \n",
    "\n",
    "•\tEl modelo BERT permite resolver diferentes problemas de procesamiento de lenguaje natural, utilizando las redes neuronales Transformers como base para su funcionamiento.\n",
    "\n",
    "•\tBERT se utiliza en diferentes aplicaciones, entre ellas resumen de textos, análisis de sentimientos, traducción de textos, generación de documentos, responder preguntas, entre otros.\n",
    "\n",
    "•\tEl desarrollo de BERT ha optimizado la búsqueda en el motor de Google y ha logrado un avance significativo en lo que se refiere a la comunicación computadora-hombre, consiguiendo que los humanos se puedan comunicar utilizando un lenguaje cada vez mas natural con las máquinas.\n",
    "\n",
    "•\tAl tratarse de un código abierto y de rápido ajuste, deja abierta una gran puerta para el desarrollo de nuevos modelos basados en BERT, como patentBERT, docBERT, RoBERTa, DestilBERT, ALBERT, entre otras.\n",
    "\n",
    "•   Implementar soluciones a problemas de PLN, hoy en dia esta mas al alcance de todos los desarrolladores gracias a la existencia de proyectos open source que usan BERT tomando ventajas de todas las capacidades que estos modelos pre-entrenados ofrecen reduciendo considerablemente el tiempo de desarrollo y el time-to-market. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed07ed1",
   "metadata": {},
   "source": [
    "## Referencias bibliográficas\n",
    "\\bibliographystyle{unsrt}\n",
    "\\bibliography{references}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa2c613",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "https://medium.com/swlh/differences-between-word2vec-and-bert-c08a3326b5d1#:~:text=Word2Vec%20will%20generate%20the%20same,vectors%20like%20beach%2C%20coast%20etc.\n",
    "\n",
    "https://www.blog.google/products/search/search-language-understanding-bert/\n",
    "\n",
    "https://huggingface.co/blog/bert-101?text=Paris+is+the+%5BMASK%5D+of+France.\n",
    "\n",
    "https://www.cyberclick.es/numerical-blog/google-bert-que-es-como-funciona-y-como-te-afecta\n",
    "\n",
    "RANKBRAIN\n",
    "https://www.abc.us.org/ojs/index.php/ei/article/view/539/1041\n",
    "\n",
    "Rafael Lucca, ¿Qué es BERT y cómo funciona?.\n",
    "https://dxmedia.net/algoritmo-bert-google/\n",
    "\n",
    "ALBERT\n",
    "https://openreview.net/pdf?id=H1eA7AEtvS\n",
    " \n",
    " \n",
    "https://towardsdatascience.com/understanding-bert-is-it-a-game-changer-in-nlp-7cca943cf3ad\n",
    " \n",
    "https://kryptonsolid.com/que-es-bert-modelo-de-lenguaje-y-como-funciona/\n",
    " \n",
    " \n",
    " \n",
    "DocBERT\n",
    "https://www.researchgate.net/publication/332493790_DocBERT_BERT_for_Document_Classification\n",
    " \n",
    " \n",
    "bioBERT\n",
    "https://arxiv.org/ftp/arxiv/papers/1901/1901.08746.pdf\n",
    " \n",
    "VideoBERT\n",
    "https://www.researchgate.net/publication/332186832_VideoBERT_A_Joint_Model_for_Video_and_Language_Representation_Learning\n",
    " \n",
    "SciBERT\n",
    "https://aclanthology.org/D19-1371.pdf\n",
    " \n",
    "G-BERT\n",
    "https://arxiv.org/abs/1906.00346\n",
    "\n",
    "\n",
    "patentBERT\n",
    "https://www.kaggle.com/datasets/danofer/patentbert\n",
    "\n",
    "Roberta\n",
    "https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/\n",
    "\n",
    "\n",
    "\n",
    "XLM-R \n",
    "https://ai.facebook.com/blog/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision/\n",
    "\n",
    "\n",
    "DestilBERT\n",
    "https://huggingface.co/docs/transformers/model_doc/distilbert\n",
    "\n",
    "Antecesores de BERT\n",
    "https://medium.com/@kashyapkathrani/all-about-embeddings-829c8ff0bf5b\n",
    "https://hmong.es/wiki/GloVe_(machine_learning)\n",
    "https://towardsdatascience.com/glove-elmo-bert-9dbbc9226934\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f47a0b",
   "metadata": {},
   "source": [
    "\\end{document}"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Erika Paola Ortiz"
   },
   {
    "name": "Romina Iglesias"
   }
  ],
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "title": "Hablemos de BERT Red Neuronal",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "95px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.852px",
    "left": "1311px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
