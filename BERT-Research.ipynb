{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b51438",
   "metadata": {},
   "source": [
    "---\n",
    "title: Hablemos de BERT\n",
    "author: Erika Paola Ortiz y Romina Soledad Iglesias \n",
    "date: today\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e930b08f",
   "metadata": {},
   "source": [
    "# Hablemos de BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5427963c",
   "metadata": {},
   "source": [
    "## Palabras clave\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf6dad7",
   "metadata": {},
   "source": [
    "- **BERT**: Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "- **PLN:** Procesamiento de Lenguaje Natural  (NLP en Inglés): Es un campo de las ciencias de la computación, de la inteligencia artificial y de la lingüística que estudia las interacciones entre las computadoras y el lenguaje humano. Se ocupa de la formulación e investigación de mecanismos eficaces computacionalmente para la comunicación entre personas y máquinas por medio del lenguaje natural, es decir, de las lenguas del mundo.\n",
    "\n",
    "- **Deep Learning:** Aprendizaje profundo (en inglés, deep learning) es un conjunto de algoritmos de aprendizaje automático (en inglés, machine learning) que intenta modelar abstracciones de alto nivel en datos usando arquitecturas computacionales que admiten transformaciones no lineales múltiples e iterativas de datos expresados en forma matricial o tensorial. \n",
    "\n",
    "* **Machine learning:** es una disciplina, dentro de la Inteligencia Artificial, donde se estudian métodos para hacer predicciones, que podamos programar y automatizar. Machine Learning dota a las máquinas de la capacidad de aprender.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4acfcbc",
   "metadata": {},
   "source": [
    "## Introducción\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe41970a",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f3ec91",
   "metadata": {},
   "source": [
    "## Objetivos y Metodología\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83dfc95",
   "metadata": {},
   "source": [
    "- Explicar el concepto de BERT. \n",
    "- Identificar las aplicaciones del modelo BERT. \n",
    "- Presentar un demo de algunas de las aplicaciones de BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946ccb95",
   "metadata": {},
   "source": [
    "## Resultados de la Investigacion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5157eb98",
   "metadata": {},
   "source": [
    "### ¿Qué es BERT?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffadcd1",
   "metadata": {},
   "source": [
    "\n",
    "BERT Bidirectional Encoder Representations from Transformers por sus siglas en Inglés en español significa Representación de Codificador Bidireccional de Transformadores. \n",
    "\n",
    "BERT funciona de manera bidireccional, es decir, analiza los textos ya sea de derecha a izquierda como de izquierda a derecha, esto permite que no solo reconozca los términos claves de una búsqueda, sino que entienda el contexto de las palabras en la frase u oración, interpretando las búsquedas de manera más precisa.\n",
    " \n",
    "BERT es un modelo de machine learning que se utiliza como una técnica para el Procesamiento de Lenguaje Natural. BERT está basada en el uso de redes neuronales Transformers. \n",
    " \n",
    " \n",
    "**Redes Neuronales Transformers**\n",
    "\n",
    "Las redes neuronales Transformers son modelos que tienen diversas aplicaciones como traducir textos, escribir poemas, sintetizar y/o generar texto, generar código fuente entre otras aplicaciones. \n",
    " \n",
    "Las redes transformers fueron creados por Google en  2017 y por la Universidad de Toronto. En el momento de su introducción, los modelos de lenguaje usaban principalmente redes neuronales recurrentes (RNN) y redes neuronales convolucionales (CNN) para manejar tareas de Procesamiento de Lenguaje Natural (PNL).\n",
    "Estos modelos aún son competentes, sin embargo, el Transformer se considera una mejora significativa ya que no requiere que las secuencias de datos se procesen en un orden fijo, mientras que las RNN y las CNN sí lo hacen. \n",
    "\n",
    "Su enfoque principal de los Transformers era realizar traducciones, pero puede ser entrenado para diferentes usos ya que poseen una alta capacidad de procesamiento en paralelo lo que los hace más eficientes reduciendo el tiempo y facilidad de entrenamiento y aumentando el tamaño de los set de datos que pueden procesar. \n",
    " \n",
    "Los transformers están basados en tres principales innovaciones: \n",
    "- Positioning encoding\n",
    "- Attention\n",
    "- Self attention. \n",
    "\n",
    "A continuación explicamos en qué consiste cada una de las tres principales innovaciones:\n",
    " \n",
    "- **Positioning Encoding**\n",
    "\n",
    "Los transformers utilizan codificadores posicionales para etiquetar elementos de datos que entran y salen de la red. \n",
    "\n",
    "El Transformer recibe una oración de entrada y la convierte en dos secuencias: una secuencia de vectores de palabras y una secuencia de codificaciones posicionales. Ambos vectores son escritos usando representaciones numéricas del texto para que la red neuronal pueda procesarlas. Cada palabra del diccionario se representa como un vector. Las codificaciones posicionales son una representación vectorial de la posición de la palabra en la oración original.\n",
    "El transformer junta ambas secuencias y pasa el resultado a través de una serie de codificadores, seguidos de una serie de decodificadores. Esto es necesario debido a que el input no es alimentado en la red de forma secuencial sino que se pasa todo de una vez.\n",
    "\n",
    " \n",
    "- **Attention**\n",
    "\n",
    "Attention es un mecanismo que permite a un modelo de texto «mirar» cada una de las palabras de la frase original al tomar una decisión sobre cómo traducir las palabras de la frase de salida.\n",
    " \n",
    "Este modelo aprende de los datos de entrenamiento. Por ejemplo, en el caso de los traductores, al ver miles de ejemplos de frases en diferentes idiomas, el modelo aprende qué tipos de palabras son interdependientes. De esta manera aprende a respetar el género, la pluralidad y otras reglas gramaticales.\n",
    " \n",
    " \n",
    "- **Self attention**\n",
    " \n",
    "El mecanismo Self attention es lo que permite al modelo saber con qué otra palabra de la oración está relacionada la palabra que se procesa en ese instante de tiempo. Es una capa que ayuda al codificador a ver otras palabras en la oración de entrada mientras codifica una palabra específica.\n",
    "\n",
    "\n",
    "![transformers-architecture.png](attachment:transformers-architecture.png)\n",
    "\n",
    "<cite data-footcite=\"Figure1\">Source: Figure 1: The Transformer - model architecture. Wasani et al.., 2017 [Paper attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)</cite>.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc74fd5a",
   "metadata": {},
   "source": [
    "## Aplicaciones de BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e67d4a",
   "metadata": {},
   "source": [
    "El modelo BERT puede ayudar a resolver problemas de procesamiento de lenguage natural como: \n",
    "- Traducción de textos. \n",
    "- Generación de documentos. \n",
    "- Resumen de textos. \n",
    "- Prediccion de preguntas y respuestas. \n",
    "- Analisis de sentimientos (por ejemplo analisis de reviews, analisis del mercado de valores en donde se analicen dos variables reviews positivas o negativas, estado  positivo o negativo del mercado de valores). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4f1f6",
   "metadata": {},
   "source": [
    "## Conclusiones\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa0a0be",
   "metadata": {},
   "source": [
    "- El modelo BERT ha ayudado a mejorar los difeerentes problemas de procesamiento de Lenguaje Natural, gracias a su enfoque bidireccional, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed07ed1",
   "metadata": {},
   "source": [
    "## Referencias bibliográficas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa2c613",
   "metadata": {},
   "source": [
    "<cite data-footcite=\"Figure1\">Source: Figure 1: The Transformer - model architecture. Wasani et al.., 2017 [Paper attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)</cite>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd8a7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b31ee25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "95px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "283px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.852px",
    "left": "1311px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
