{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b51438",
   "metadata": {},
   "source": [
    "---\n",
    "title: Hablemos de BERT\n",
    "author: Erika Paola Ortiz y Romina Soledad Iglesias \n",
    "date: today\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e930b08f",
   "metadata": {},
   "source": [
    "# Hablemos de BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5427963c",
   "metadata": {},
   "source": [
    "## Palabras clave\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf6dad7",
   "metadata": {},
   "source": [
    "- **BERT**: Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "- **PLN:** Procesamiento de Lenguaje Natural  (NLP en Inglés): Es un campo de las ciencias de la computación, de la inteligencia artificial y de la lingüística que estudia las interacciones entre las computadoras y el lenguaje humano. Se ocupa de la formulación e investigación de mecanismos eficaces computacionalmente para la comunicación entre personas y máquinas por medio del lenguaje natural, es decir, de las lenguas del mundo.\n",
    "\n",
    "- **Deep Learning:** Aprendizaje profundo (en inglés, deep learning) es un conjunto de algoritmos de aprendizaje automático (en inglés, machine learning) que intenta modelar abstracciones de alto nivel en datos usando arquitecturas computacionales que admiten transformaciones no lineales múltiples e iterativas de datos expresados en forma matricial o tensorial. \n",
    "\n",
    "* **Machine learning:** es una disciplina, dentro de la Inteligencia Artificial, donde se estudian métodos para hacer predicciones, que podamos programar y automatizar. Machine Learning dota a las máquinas de la capacidad de aprender.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4acfcbc",
   "metadata": {},
   "source": [
    "## Introducción\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe41970a",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f3ec91",
   "metadata": {},
   "source": [
    "## Objetivos y Metodología\t\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83dfc95",
   "metadata": {},
   "source": [
    "- Explicar el concepto de BERT. \n",
    "- Identificar las aplicaciones del modelo BERT. \n",
    "- Presentar un demo de algunas de las aplicaciones de BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946ccb95",
   "metadata": {},
   "source": [
    "## Resultados de la Investigacion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5157eb98",
   "metadata": {},
   "source": [
    "### ¿Qué es BERT?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffadcd1",
   "metadata": {},
   "source": [
    "\n",
    "BERT Bidirectional Encoder Representations from Transformers por sus siglas en Inglés en español significa Representación de Codificador Bidireccional de Transformadores. \n",
    "\n",
    "BERT es un modelo de machine learning de código abierto desarrollado por investigadores de Google en 2018 que se utiliza como una técnica para el Procesamiento de Lenguaje Natural.\n",
    "\n",
    "BERT funciona de manera bidireccional, es decir, analiza los textos ya sea de derecha a izquierda como de izquierda a derecha, esto permite que no solo reconozca los términos claves de una búsqueda, sino que entienda el contexto de las palabras en la frase u oración, interpretando las búsquedas de manera más precisa. BERT está basada en el uso de redes neuronales Transformers. \n",
    "\n",
    " \n",
    "**Redes Neuronales Transformers**\n",
    "\n",
    "Las redes neuronales Transformers son modelos que tienen diversas aplicaciones como traducir textos, escribir poemas, sintetizar y/o generar texto, generar código fuente entre otras aplicaciones. \n",
    " \n",
    "Las redes transformers fueron creados por Google en  2017 y por la Universidad de Toronto. En el momento de su introducción, los modelos de lenguaje usaban principalmente redes neuronales recurrentes (RNN) y redes neuronales convolucionales (CNN) para manejar tareas de Procesamiento de Lenguaje Natural (PNL).\n",
    "Estos modelos aún son competentes, sin embargo, el Transformer se considera una mejora significativa ya que no requiere que las secuencias de datos se procesen en un orden fijo, mientras que las RNN y las CNN sí lo hacen. \n",
    "\n",
    "Su enfoque principal de los Transformers era realizar traducciones, pero puede ser entrenado para diferentes usos ya que poseen una alta capacidad de procesamiento en paralelo lo que los hace más eficientes reduciendo el tiempo y facilidad de entrenamiento y aumentando el tamaño de los set de datos que pueden procesar. \n",
    " \n",
    "Los transformers están basados en tres principales innovaciones: \n",
    "- Positioning encoding\n",
    "- Attention\n",
    "- Self attention. \n",
    "\n",
    "A continuación explicamos en qué consiste cada una de las tres principales innovaciones:\n",
    " \n",
    "- **Positioning Encoding**\n",
    "\n",
    "Los transformers utilizan codificadores posicionales para etiquetar elementos de datos que entran y salen de la red. \n",
    "\n",
    "El Transformer recibe una oración de entrada y la convierte en dos secuencias: una secuencia de vectores de palabras y una secuencia de codificaciones posicionales. Ambos vectores son escritos usando representaciones numéricas del texto para que la red neuronal pueda procesarlas. Cada palabra del diccionario se representa como un vector. Las codificaciones posicionales son una representación vectorial de la posición de la palabra en la oración original.\n",
    "El transformer junta ambas secuencias y pasa el resultado a través de una serie de codificadores, seguidos de una serie de decodificadores. Esto es necesario debido a que el input no es alimentado en la red de forma secuencial sino que se pasa todo de una vez.\n",
    "\n",
    " \n",
    "- **Attention**\n",
    "\n",
    "Attention es un mecanismo que permite a un modelo de texto «mirar» cada una de las palabras de la frase original al tomar una decisión sobre cómo traducir las palabras de la frase de salida.\n",
    " \n",
    "Este modelo aprende de los datos de entrenamiento. Por ejemplo, en el caso de los traductores, al ver miles de ejemplos de frases en diferentes idiomas, el modelo aprende qué tipos de palabras son interdependientes. De esta manera aprende a respetar el género, la pluralidad y otras reglas gramaticales.\n",
    " \n",
    " \n",
    "- **Self attention**\n",
    " \n",
    "El mecanismo Self attention es lo que permite al modelo saber con qué otra palabra de la oración está relacionada la palabra que se procesa en ese instante de tiempo. Es una capa que ayuda al codificador a ver otras palabras en la oración de entrada mientras codifica una palabra específica.\n",
    "\n",
    "\n",
    "![transformers-architecture.png](attachment:transformers-architecture.png)\n",
    "\n",
    "<cite data-footcite=\"Figure1\">Source: Figure 1: The Transformer - model architecture. Wasani et al.., 2017 [Paper attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)</cite>.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed342fa6",
   "metadata": {},
   "source": [
    "## Entrenamiento de BERT\n",
    "\n",
    "Como ya mencionamos anteriormente, BERT funciona de manera bidireccional, esto permite que no solo reconozca los términos claves de una búsqueda, sino que entienda el contexto de las palabras en la frase u oración. \n",
    "Para lograr esto, el entrenamiento de BERT se basa en dos estrategias:\n",
    "- **Mask Language Model (MLM)**, consiste en enmascar algunas de las palabras de la entrada y luego se condiciona cada palabra bidireccionalmente para predecir las palabras enmascaradas. Para ello, al tomar una oración, el modelo enmascara al azar el 15% de las palabras en la entrada. Luego, el modelo intenta predecir el valor original de las palabras enmascaradas, basándose en el contexto proporcionado por las otras palabras no enmascaradas de la secuencia.\n",
    "\n",
    "- **Predicción de la siguiente oración (Next Sentence Prediction, NSP)**, en esta etapa, BERT aprende a modelar las relaciones entre oraciones. Para ello, en el proceso de entrenamiento, el modelo recibe pares de oraciones como entrada y aprende a predecir si la segunda oración del par es la oración posterior del documento original. Es decir, si se tienen dos oraciones A y B, BERT debe descifrar si B es la siguiente oración real que viene después de A en el corpus, o es solo una oración aleatoria.\n",
    "\n",
    "Al entrenar el modelo BERT, ambas técnicas se entrenan juntas, de esta forma, se minimiza la función de pérdida combinada de las dos estrategias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc74fd5a",
   "metadata": {},
   "source": [
    "## Aplicaciones de BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e67d4a",
   "metadata": {},
   "source": [
    "El modelo BERT puede ayudar a resolver problemas de procesamiento de lenguage natural como: \n",
    "- Traducción de textos. \n",
    "- Generación de documentos. \n",
    "- Resumen de textos. \n",
    "- Prediccion de preguntas y respuestas. \n",
    "- Analisis de sentimientos (por ejemplo analisis de reviews, analisis del mercado de valores en donde se analicen dos variables reviews positivas o negativas, estado  positivo o negativo del mercado de valores). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd43247",
   "metadata": {},
   "source": [
    "## Desarrollos posteriores a BERT\n",
    "\n",
    "Algunos de los desarrollos que se produjeron luego de la presentación de BERT son los siguientes:\n",
    "- **patentBERT** Desarrollado por Jieh-Sheng Lee y Jieh Hsiang, es un modelo BERT ajustado para realizar la clasificación de patentes.\n",
    "- **docBERT** Desarrollado por Raphael Tang y Jimmy Lin, es un modelo BERT perfeccionado para la clasificación de documentos.\n",
    "- **bioBERT** Desarrollado por Jinhyuk Lee y Wonjin Yoon, es un modelo de representación de lenguaje biomédico previamente entrenado para la minería de textos biomédicos.\n",
    "- **VideoBERT** Desarrollado por Chen Sun y Carl Vondrick, es un modelo visual-lingüístico conjunto para procesar el aprendizaje no supervisado de una gran cantidad de datos sin etiquetar en Youtube.\n",
    "- **SciBERT** Desarrollado por Iz Beltag, Kyle Lo y Arman Cohan, es un modelo BERT previamente entrenado para texto científico.\n",
    "- **G-BERT** Desarrollado por Junyuan Shang, Tengfei Ma, Cao Xiao y  Jimeng Sun, es un modelo BERT previamente entrenado usando códigos médicos con representaciones jerárquicas usando redes neuronales gráficas (GNN) y luego ajustado para hacer recomendaciones médicas.\n",
    "- **RoBERTa** Desarrollado por Facebook, RoBERTa se basa en la estrategia de enmascaramiento del lenguaje de BERT y modifica algunos de los hiperparámetros clave en BERT. Para mejorar el procedimiento de entrenamiento, RoBERTa elimina la tarea de predicción de la siguiente oración (NSP) del preentrenamiento de BERT e introduce un enmascaramiento dinámico para que el token enmascarado cambie durante las épocas de entrenamiento. También se entrenó en un orden de magnitud más datos que BERT, durante un período de tiempo más largo.\n",
    "- **DistilBERT** Desarrollado por HuggingFace, es un modelo de transformadores, más pequeño y más rápido que BERT, que fue entrenado previamente en el mismo corpus de una manera autosupervisada, utilizando el modelo base BERT como profesor. Esto significa que fue entrenado previamente solo en los textos en bruto, sin que los humanos los etiquetaran de ninguna manera (por lo que puede usar muchos datos disponibles públicamente) con un proceso automático para generar entradas y etiquetas a partir de esos textos utilizando el modelo base BERT.Tiene un 40 % menos de parámetros que bert-base sin recaudación, funciona un 60 % más rápido mientras preserva más del 95 % de los rendimientos de BERT medidos en el punto de referencia de comprensión del lenguaje GLUE.\n",
    "- **XLM-R** Desarrollado por Facebook, es un nuevo modelo que utiliza técnicas de entrenamiento autosupervisadas para lograr un rendimiento de última generación en la comprensión interlingüe, una tarea en la que un modelo se entrena en un idioma y luego se utiliza con otros idiomas sin datos de entrenamiento adicionales. Nuestro modelo mejora los enfoques multilingües anteriores al incorporar más datos e idiomas de capacitación, incluidos los llamados lenguajes de bajos recursos, que carecen de extensos conjuntos de datos etiquetados y no etiquetados. Este modelo supera significativamente a BERT multilingüe (mBERT) en una variedad de puntos de referencia multilingües.\n",
    "- **ALBERT (A Lite BERT for Self-Supervised Learning of Language Representations)** Desarrollado conjuntamente por Google Research y el Instituto Tecnológico Toyota,está preparado para ser el sucesor de BERT, presenta dos técnicas de reducción de parámetros para reducir el consumo de memoria y aumentar la velocidad de entrenamiento de BERT. Además, también tiene su propio método de entrenamiento llamado Predicción de Orden de Senunencia (SOP) que se centra en la coherencia entre oraciones y está diseñado para abordar la ineficacia de la pérdida de predicción de la siguiente oración (NSP) propuesta en el BERT original.\n",
    "\n",
    "\n",
    "Como puede observarse, se están desarrollando muchos modelos basados en BERT, esto se debe principalmente al hecho de que es un modelo de código abierto, al cual, realizando un ajuste fino, permite obtener una amplia cantidad de aplicaciones prácticas.\n",
    "Esto genera un gran avance en el uso del aprendizaje automático para el procesamiento del lenguaje natural. Las máquinas ahora pueden entender mejor el habla y responder de forma inteligente en tiempo real. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec4f1f6",
   "metadata": {},
   "source": [
    "## Conclusiones\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa0a0be",
   "metadata": {},
   "source": [
    "- El modelo BERT ha ayudado a mejorar los diferentes problemas de procesamiento de Lenguaje Natural, gracias a su enfoque bidireccional, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed07ed1",
   "metadata": {},
   "source": [
    "## Referencias bibliográficas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa2c613",
   "metadata": {},
   "source": [
    "<cite data-footcite=\"Figure1\">Source: Figure 1: The Transformer - model architecture. Wasani et al.., 2017 [Paper attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)</cite>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd8a7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b31ee25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "95px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "283px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.852px",
    "left": "1311px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
